---
layout: post
title: Decaton - High throughput asynchronous task processing based on Apache Kafka - 2
subheading: 
author: taehyeok-jang
categories: [stream-processing]
tags: [stream-processing, kafka, decaton]
---

지난 글에서는 Kafka consumer에서 왜 동시처리가 필요한지, consumer 내 동시처리를 구현한 Confluent 사의 사례와 그 한계점을 알아보았습니다. 그리고 consumer의 성능을 최대로 하기 위해서는 결국 한 partition 내에서도 동시처리가 필요하다는 결론을 얻었습니다. 

이번 글에서는 Decaton에서 동일한 문제를 어떻게 접근하고 있는지, 단일 partition 내 동시처리 모델을 어떻게 설계했는지에 대해 알아보고 세부 구현에 대해서도 살펴보겠습니다.



## Why Decaton? 

지난 글과 마찬가지로 I/O intensive한 task를 처리하는 consumer를 예를 들겠습니다. consumer에서는 task를 consumer하여 외부 시스템의 API를 호출합니다. 이 API 호출에 시간이 오래 소요되는 탓에 consumer group 내 각 consumer에서는 긴 wait time을 가지게 되고 이는 결국 전체적인 메시지 처리 성능 저하로 이어집니다. 만약 consumer group의 병렬성이 partition의 개수와 같고, 외부 시스템은 더 높은 빈도의 API 호출을 수용할 수 있다면 현재 시스템 상에서는 consumer group이 유일한 병목입니다. 이 문제를 시스템 운영 상의 관점으로 보면 아래와 같습니다. 

- 사용자 문제 
  - 사용자가 혼자서 concurrency를 조정할 수 없습니다. 일반적으로 카프카 클러스터의 안정적 운영을 위해서는 적절한 정책과 capacity planning을 통해 시스템 관리자가 partition 개수를 조절합니다.
  - 시스템의 초기에는 요구되는 concurrency를 측정하기가 어렵습니다. 하지만 partition은 축소시키는 것이 불가능하고 partition 수를 늘리는 것은 부작용이 야기합니다.
- 시스템 관리자의 문제 
  - 더 많은 partition은 더많은 SW/HW 리소스를 사용합니다. 
  - producer는 이전보다 더 작은 크기의 batch를 발생시키며 이는 broker 입장에서 비효율적입니다. 
  - Kafka 1.1.0에서 아주 많은 수의 partition를 가지는 것에 대한 큰 개선이 있었지만, 여전히 partition은 replica assignment의 기본 단위이기 때문에 많은 수의 partition을 가지는 것은 reassignment 과정을 느리게 합니다. 



Decaton은 위 문제를 해결하기 위해 다음과 같은 개선을 이루었습니다. 

- 단일 partition 내 동시처리를 지원합니다
  - Sliding offset commit을 통해 동시처리에 따른 메시지 처리 누락을 방지합니다
  - 동일한 key의 메시지들을 순서대로 처리하는 것을 보장합니다
- Dynamic property configuration으로 한 partition 내 concurrency를 동적으로 변경 가능합니다 



## Sliding Offset Commit Strategy

Decaton의 offset commit 전략을 알아보겠습니다. 단일 partition 내 동시처리를 가능하게 하기 위해서는 offset commit coordination이 필요합니다. Kafka의 offset commit은 watermark 방식으로 동작합니다. 예를 들어 5번 record를 commit 하면 이는 1~5번 record 모두 commit 하는 것입니다. 만약 1번 record를 처리하는 데 실패했거나 지연되고 있는데도 5번 record를 commit 한다면 1번 record가 처리되지 않고 유실될 수 있습니다. 단일 스레드 모델에서는 메시지들이 순차적으로 처리되기 때문에 상관 없지만, 여러 메시지들을 동시에 처리하는 동시처리 모델에서는 이 같은 일이 발생할 수 있습니다. 따라서 동시처리 모델에서 처리 결과를 보장하기 위해서는 5번 record를 commit 하기 전에 이전의 모든 record들이 처리가 완료된 것을 확인한 다음에 commit 해야 합니다. 

Decaton에서는 offset commit coordination을 위하여 sliding offeset range 방식을 사용했습니다. Decaton에서는 각 partition 별로 큐를 가지고 있으면서 각 메시지의 offset과 처리 완료 여부를 저장합니다. 그리고 주기적으로 commit을 시도할 때 큐의 가장 앞에 있는 offset부터 처리가 완료된 offset까지를 commit 합니다. 아래 그림을 통해 동작 방식을 이해해보겠습니다.

<img src="https://user-images.githubusercontent.com/31732943/154827822-09028555-992d-412b-98ec-2bbdb4389e56.png" alt="decaton-offset-commit-01" style="zoom: 67%;" />

1. 1번 메시지가 추가되었습니다.
2. 1번 메시지가 완료되었고 주기적 commit 시도 시에 commit 되어 큐에서 제거되었습니다.
<img src="https://user-images.githubusercontent.com/31732943/154827820-000617cb-4862-4fb6-9646-8f0ac9ad7285.png" alt="decaton-offset-commit-02" style="zoom:67%;" />

3. 2~5번 메시지가 추가되었습니다. 
4. 3~5번 메시지가 완료되었지만 2번이 완료되지 않았기 때문에 주기적 commit 시도 시에 어떤 메시지도 commit 되지 않았습니다. 
<img src="https://user-images.githubusercontent.com/31732943/154827819-c0f85af6-374c-4249-b0a9-abb435855ec1.png" alt="decaton-offset-commit-03" style="zoom:67%;" />

5. 6~7번 메시지가 추가되었고 2번 메시지가 완료되었습니다.
6. 주기적 commit 시도 시에 2번 메시지와 함께 3~6번 메시지도 완료되었기 때문에 commit 되었습니다.
7. 큐에는 7번 메시지만 남아 처리 중입니다. 



위 sliding offset range 동작 방식을 Decaton에서는 OutOfOrderCommitControl 클래스에서 dequeue을 활용하여 구현하였습니다. 

[https://github.com/line/decaton/blob/master/processor/src/main/java/com/linecorp/decaton/processor/runtime/internal/OutOfOrderCommitControl.java#L90-L126](https://github.com/line/decaton/blob/master/processor/src/main/java/com/linecorp/decaton/processor/runtime/internal/OutOfOrderCommitControl.java#L90-L126)





## Overall Design 

아래는 Decaton의 전체적인 아키텍쳐를 표현하고 있습니다. 한 topic으로부터 메시지들을 처리하는 과정의 핵심을 담고 있습니다. 한 partition 내에서도 key를 기반으로 sub-partition이 이루어지며 sub-partition 내 각각의 스레드가 메시지들을 처리하고 있습니다. 

![internal-queuing](https://user-images.githubusercontent.com/31732943/154827818-82897023-fc48-4747-ad43-e0e16d5fb145.png)

[https://github.com/line/decaton/blob/master/docs/why-decaton.adoc](https://github.com/line/decaton/blob/master/docs/why-decaton.adoc)



## Implementation 

Decaton의 세부 구현에 대해서 살펴보겠습니다. 처음에는 다소 복잡하게 느껴질 수 있지만 Decaton의 핵심인 multi-threaded partition processing과 sliding offset commit 전략을 잘 알고 이를 구현한 주요 클래스들을 살펴보면 이해가 한층 더 쉽게 될 것입니다. 글에서는 주요 클래스들의 코드를 추려서 정리하였습니다. 



### DecatonProcessor

Decaton에서 구현한 consumer를 사용하려면 DecatonProcessor 인터페이스를 상속한 클래스를 서버 인스턴스 실행 시 ProcessorSubsciption에 등록해야 합니다. 여러 프로세서를 등록하여 메시지를 파이프라인 방식으로 처리하거나 재시도를 위한 프로세서를 등록하여 fallback 처리할 수 있습니다. 

```java
public class PrintMessageTaskProcessor implements DecatonProcessor<PrintMessageTask> {
    @Override
    public void process(ProcessingContext<PrintMessageTask> context, PrintMessageTask task) throws InterruptedException {
        System.out.printf("Noticed %s is %d years old\n", task.getName(), task.getAge());
    }
}
```

```java
public final class ProcessorMain {
    public static void main(String[] args) throws Exception {
        Properties consumerConfig = new Properties();
        consumerConfig.setProperty(ConsumerConfig.CLIENT_ID_CONFIG, "my-decaton-processor");
        consumerConfig.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,
                                   System.getProperty("bootstrap.servers"));
        consumerConfig.setProperty(ConsumerConfig.GROUP_ID_CONFIG, "my-decaton-processor");

        ProcessorSubscription subscription =
                SubscriptionBuilder.newBuilder("my-decaton-processor") // (1)
                                   .processorsBuilder(
                                           ProcessorsBuilder.consuming(
                                                   "my-decaton-topic",
                                                   new ProtocolBuffersDeserializer<>(PrintMessageTask.parser()))
                                                            .thenProcess(new PrintMessageTaskProcessor())
                                   )
                                   .consumerConfig(consumerConfig)
                                   .buildAndStart();

        Thread.sleep(10000);
        subscription.close();
    }
}
```



#### ProcessingContext

ProcessingContext는 프로세서 내 실행 컨텍스트를 저장하며 처리 방식을 정하기 위한 인터페이스를 지원합니다. 가장 중요하게 살펴보아야 할 인터페이스는 deferCompletion입니다. deferCompletion은 메시지의 완료 처리를 이후로 미루는 것을 의미하며 이 인터페이스를 호출하면 현재 처리 중인 메시지는 비동기로 처리하고 다음 메시지의 처리를 시작할 수 있습니다. 이때 반드시 지켜야 할 것은 deferCompletion 한 메시지를 처리하면 complete 혹은 retry를 호출하여 완료 여부를 알려야 한다는 것입니다.

```java
public interface ProcessingContext<T> {

  TaskMetadata metadata();

  String key();

  default Completion deferCompletion() {
    return deferCompletion(comp -> TimeoutChoice.GIVE_UP);
  }

  Completion deferCompletion(Function<Completion, TimeoutChoice> callback);

  Completion push(T task) throws InterruptedException;

  Completion retry() throws InterruptedException;
}
```



### ProcessorSubscription

ProcessorSubscription는 topic을 subscribe하고, 메시지를 처리하기 위한 전체 과정을 담고 있는 class입니다. 

1. 프로세서를 실행하여 브로커로부터 partition이 할당되면 AssignmentManager를 통해 각 partition을 처리하기 위한 실행 컨텍스트인 PartitionContext를 생성합니다. 
2. ConsumeManager는 메시지들을 poll하여 처리합니다. 
   1. 각 메시지는 자신의 partition에 대응되는 PartitionContext 내에서 처리가 됩니다. 
   2. 메시지들을 동기 혹은 비동기 방식으로 처리한 다음 각 PartitionContext는 high watermark를 업데이트합니다. 
   3. 특정 partition에서 처리가 지연되는 상황에서는 메시지들을 더 가져와도 consume이 불가능합니다. ConsumerManager는 각 PartitionContext가 max.pending.records을 초과하는 레코드들을 처리하고 있으면 해당 partition은 다음 poll 때 메시지들을 가져오지 않도록 합니다.
3. CommitManager는 마지막 commit으로부터 commit.interval.ms 만큼의 시간이 지나면 비동기 commit을 합니다. 각 partition 별로 commit 할 offset은 PartitionContext의 high watermark offset입니다. 
5. 만약 decaton.partition.concurrency 값이 변경되었다면 이전 실행 컨텍스트들은 삭제하고 해당 개수만큼 실행 컨텍스트를 다시 생성합니다. 



```java
public class ProcessorSubscription extends Thread implements AsyncShutdownable {
  @Override 
  public void run() {
    started = true;
    try {
      if (!terminated) {
        updateState(SubscriptionStateListener.State.INITIALIZING);
        consumeManager.init(subscribeTopics());
        consumeLoop(); 
      }
    } finally {
      loopTerminateFuture.complete(null);
    }
  }

  private void consumeLoop() {
    try {
      while (!terminated) {
        consumeManager.poll();
        contexts.maybeHandlePropertyReload();

        commitManager.maybeCommitAsync();
      }
  }
}
```



#### AssignmentManager

```java
public class AssignmentManager {
  
  private final AssignmentStore store;

  public void assign(Collection<TopicPartition> newAssignment) {
    Set<TopicPartition> newSet = new HashSet<>(newAssignment);
    Set<TopicPartition> oldSet = store.assignedPartitions();
    List<TopicPartition> removed = computeRemovedPartitions(oldSet, newSet);
    List<TopicPartition> added = computeAddedPartitions(oldSet, newSet);

    partitionsRevoked(removed);
    partitionsAssigned(added);
  }

  private void partitionsRevoked(Collection<TopicPartition> partitions) {
    if (partitions.isEmpty()) {
      return;
    }
    store.removePartition(partitions);
  }

  private void partitionsAssigned(Collection<TopicPartition> partitions) {
    if (partitions.isEmpty()) {
      return;
    }
    Map<TopicPartition, AssignmentConfig> configs =
      partitions.stream().collect(toMap(Function.identity(), ignored -> new AssignmentConfig(false)));
    store.addPartitions(configs);
  }

}
```



#### ConsumeManager

```java
public class ConsumeManager implements AutoCloseable {
  public void poll() {
    ConsumerRecords<String, byte[]> records = consumer.poll(POLL_TIMEOUT_MILLIS);
    records.forEach(handler::receive);

    states.updatePartitionsStatus();

    pausePartitions(consumer);
    resumePartitions(consumer);
  }

  public void init(Collection<String> subscribeTopics) {
    Set<TopicPartition> pausedPartitions = new HashSet<>();
    consumer.subscribe(subscribeTopics, new ConsumerRebalanceListener() {
      @Override
      public void onPartitionsRevoked(Collection<TopicPartition> partitions) {
        if (consumerClosing.get()) {
          return;
        }

        handler.prepareForRebalance();
        pausedPartitions.addAll(consumer.paused());
      }

      @Override
      public void onPartitionsAssigned(Collection<TopicPartition> partitions) {
        handler.updateAssignment(partitions);

        pausedPartitions.retainAll(partitions);
        try {
          consumer.pause(pausedPartitions);
        } finally {
          pausedPartitions.clear();
        }
      }
    });
  }
}
```



#### CommitManager 

```java
public class CommitManager { 
  public void maybeCommitAsync() {
    if (clock.millis() - lastCommittedMillis >= commitIntervalMillis.value()) {
      try {
        commitAsync();
      } finally {
        lastCommittedMillis = clock.millis();
      }
    }
  }

  public void commitAsync() {
    Map<TopicPartition, OffsetAndMetadata> offsets = store.commitReadyOffsets();
    if (offsets.isEmpty()) {
      return;
    }

    if (asyncCommitInFlight) {
      return;
    }
    Thread callingThread = Thread.currentThread();
    consumer.commitAsync(offsets, (ignored, exception) -> {
      asyncCommitInFlight = false;
      if (exception != null) {
        return;
      }
      if (Thread.currentThread() != callingThread) {
        return;
      }
      store.storeCommittedOffsets(offsets);
    });
    asyncCommitInFlight = true;
  }
}
```



### PartitionContext 

PartitionContext는 한 Partition의 실행 컨텍스트의 모든 상태를 저장하고 있습니다. 어떤 partition을 처리하는지를 저장하는 PartitionScope, 메시지들을 실질적으로 처리하는 PartitionProcessor, 그리고 sliding offset commit을 관리하기 위한 OutOfOrderCommitControl을 가지고 있습니다. 

```java
public class PartitionContext implements AutoCloseable {
  private final PartitionScope scope;
  private final PartitionProcessor partitionProcessor;
  private final OutOfOrderCommitControl commitControl;
  private final Processors<?> processors;

  public void addRequest(TaskRequest request) {
    partitionProcessor.addTask(request);
    if (lastQueueStarvedTime > 0) {
      metrics.queueStarvedTime.record(System.nanoTime() - lastQueueStarvedTime, TimeUnit.NANOSECONDS);
      lastQueueStarvedTime = -1;
    }
  }

  public void updateHighWatermark() {
    commitControl.updateHighWatermark();
    int pendingCount = commitControl.pendingOffsetsCount();
    if (pendingCount == 0 && lastQueueStarvedTime < 0) {
      lastQueueStarvedTime = System.nanoTime();
    }
  }

  public OptionalLong offsetWaitingCommit() {
    long readyOffset = commitControl.commitReadyOffset();
    if (readyOffset > lastCommittedOffset) {
      return OptionalLong.of(readyOffset);
    }
    return OptionalLong.empty();
  }

  public void updateCommittedOffset(long offset) {
    lastCommittedOffset = offset;
  }
}
```





### PartitionProcessor

PartitionProcessor는 메시지들을 실질적으로 처리하는 클래스입니다. 초기화 시에 Decaton의 decaton.partition.concurrency만큼 ProcessorUnit을 생성하여 동시처리를 위해 사용합니다. 처리할 task가 유입되면 PartitionProcessor는 key를 기반으로 sub-partitioning을 하여 해당 메시지들을 처리할 ProcessorUnit에 할당합니다. 


```java
public class PartitionProcessor implements AsyncShutdownable {
  private final PartitionScope scope;
  private final Processors<?> processors;
  private final List<ProcessorUnit> units;
  private final SubPartitioner subPartitioner;

  private final RateLimiter rateLimiter;

  public PartitionProcessor(PartitionScope scope, Processors<?> processors) {
    this.scope = scope;
    this.processors = processors;

    int concurrency = scope.props().get(ProcessorProperties.CONFIG_PARTITION_CONCURRENCY).value();
    units = new ArrayList<>(concurrency);
    subPartitioner = new SubPartitioner(concurrency);
    rateLimiter = new DynamicRateLimiter(scope.props().get(ProcessorProperties.CONFIG_PROCESSING_RATE));
    for (int i = 0; i < concurrency; i++) {
      units.add(createUnit(i));
    }
  }

  ProcessorUnit createUnit(int threadId) {
    ThreadScope threadScope = new ThreadScope(scope, threadId);
    ExecutionScheduler scheduler = new ExecutionScheduler(threadScope, rateLimiter);

    TopicPartition tp = scope.topicPartition();
    ProcessPipeline<?> pipeline = processors.newPipeline(threadScope, scheduler, metrics);
    return new ProcessorUnit(threadScope, pipeline);
  }

  public void addTask(TaskRequest request) {
    int subPartition = subPartitioner.partitionFor(request.key());
    units.get(subPartition).putTask(request);
  }
}
```



#### ProcessorUnit, ProcessPipeline

ProcessorUnit은 각 sub-partition 당 메시지 처리를 위한 클래스입니다. 비동기 처리를 위한 executor를 가지고 있으며 메시지를 처리하기 위한 프로세서 리스트를 ProcessPipeline 형태로 가지고 있습니다. ProcessPipeline에서는 할당된 메시지를 처리합니다. 만약 메시지 내 metadata에 처리 대기시간이 schedule 되어 있으면 그만큼 sleep 한 다음 처리를 합니다. 프로세서의 ProcessingContext에서 주어진 메시지의 완료 처리를 미루었다면 (defer completion) deferred.complete.timeout.ms 만큼의 timeout을 걸어 처리 완료 여부를 감시합니다. 

```java
public class ProcessorUnit implements AsyncShutdownable {
  private final ProcessPipeline<?> pipeline;
  private final ExecutorService executor;

  public void putTask(TaskRequest request) {
    executor.execute(() -> processTask(request));
  }

  private void processTask(TaskRequest request) {
    pipeline.scheduleThenProcess(request);
  }
}
```

```java
public class ProcessPipeline<T> implements AutoCloseable {

  public void scheduleThenProcess(TaskRequest request) throws InterruptedException {
        OffsetState offsetState = request.offsetState();
        final DecatonTask<T> extracted = extract(request);

        scheduler.schedule(extracted.metadata());

        Completion result = process(request, extracted);
        offsetState.completion().completeWith(result);
        long timeoutMs = scope.props().get(ProcessorProperties.CONFIG_DEFERRED_COMPLETE_TIMEOUT_MS).value();
        if (timeoutMs >= 0) {
            long expireAt = clock.millis() + timeoutMs;
            offsetState.setTimeout(expireAt);
        }
    }
}
```





## References

- Kafka
  - [https://kafka.apache.org](https://kafka.apache.org)
  - [https://dzone.com/articles/kafka-topic-architecture-replication-failover-and](https://dzone.com/articles/kafka-topic-architecture-replication-failover-and)
- Decaton GitHub
  - [https://github.com/line/decaton](https://github.com/line/decaton)
  - [https://github.com/line/decaton/blob/master/docs/why-decaton.adoc](https://github.com/line/decaton/blob/master/docs/why-decaton.adoc)
- LINE Engineering Blog 
  - [https://engineering.linecorp.com/ko/blog/how-to-use-kafka-in-line-1/](https://engineering.linecorp.com/ko/blog/how-to-use-kafka-in-line-1/)
  - [https://engineering.linecorp.com/ko/blog/how-to-use-kafka-in-line-2/](https://engineering.linecorp.com/ko/blog/how-to-use-kafka-in-line-2/)
  - [https://engineering.linecorp.com/ko/blog/how-to-use-kafka-in-line-3/](https://engineering.linecorp.com/ko/blog/how-to-use-kafka-in-line-3/)
  - [https://engineering.linecorp.com/ko/blog/decaton-case-studies/](https://engineering.linecorp.com/ko/blog/decaton-case-studies/)
  - [https://speakerdeck.com/line_devday2020/decaton-high-performance-task-processing-with-kafka?slide=16](https://speakerdeck.com/line_devday2020/decaton-high-performance-task-processing-with-kafka?slide=16)

- [TCP sliding window with error animation](https://www.youtube.com/watch?v=lk27yiITOvU)







