<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="/jekyll-theme-yat/feed.xml" rel="self" type="application/atom+xml" /><link href="/jekyll-theme-yat/" rel="alternate" type="text/html" /><updated>2024-01-14T09:18:29+00:00</updated><id>/jekyll-theme-yat/feed.xml</id><title type="html">Keep Calm and HAKUNA MATATA</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><author><name>TaeHyeok Jang</name></author><entry><title type="html">CSE6250 Big Data for Healthcare - 1. Intro</title><link href="/jekyll-theme-yat/gatech/2024/01/08/cse6250-1-intro-to-bd4h.html" rel="alternate" type="text/html" title="CSE6250 Big Data for Healthcare - 1. Intro" /><published>2024-01-08T00:00:00+00:00</published><updated>2024-01-08T00:00:00+00:00</updated><id>/jekyll-theme-yat/gatech/2024/01/08/cse6250-1-intro-to-bd4h</id><content type="html" xml:base="/jekyll-theme-yat/gatech/2024/01/08/cse6250-1-intro-to-bd4h.html"><![CDATA[<h2 id="about-bd4h">About BD4H</h2>

<p>This course exist at intersection of Healthcare and Big Data.</p>

<ul>
  <li>Healthcare applications/dataset</li>
  <li>Data science, Big data analytics (algorithms, systems for processing and analyzing big data)</li>
</ul>

<h2 id="learning-goals">Learning Goals</h2>

<ul>
  <li>Understand healthcare data</li>
  <li>Understand different analytics algorithms</li>
  <li>Understand big data systems</li>
</ul>

<p>Throughout the course, we build models on health care data (ex. models for individual disease risk prediction, recommending treatments, cluster patients )</p>

<h2 id="current-problems-in-healthcare">Current Problems in Healthcare</h2>

<p>Healthcare industry is huge, and there are a lot of data coming out of health care.</p>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/032df45e-9c1c-45a3-b364-5881a9daf81f" alt="cse6250_01_01" /></p>

<p>U.S. health care is incredibly expensive! 3.8T$ per year.</p>

<p>There is a massive waste in health care. 765B$ per year.</p>

<ul>
  <li>High cost</li>
  <li>High waste</li>
  <li>Low quality</li>
</ul>

<p>Preventable deaths rank no.3 of causes of death, following cancer and heart disease.</p>

<h2 id="the-four-vs">The Four Vs</h2>

<ul>
  <li>Volume (ex. human genome 200GB raw data, single fMRI 300GB)</li>
  <li>Variety (ex. clinical, patient generated date. On-device)</li>
  <li>Velocity (coming in real-time, need to be processed and analyzed. Ex. blood pressure, temperature, heat rate)</li>
  <li>Veracity (a lot of noises, missing data, errors, false alarms)</li>
</ul>]]></content><author><name>taehyeok-jang</name></author><category term="gatech" /><category term="gatech" /><category term="big-data" /><category term="machine-learning" /><summary type="html"><![CDATA[About BD4H]]></summary></entry><entry><title type="html">CSE6250 Big Data for Healthcare - 2. Overview</title><link href="/jekyll-theme-yat/gatech/2024/01/08/cse6250-2-course-overview.html" rel="alternate" type="text/html" title="CSE6250 Big Data for Healthcare - 2. Overview" /><published>2024-01-08T00:00:00+00:00</published><updated>2024-01-08T00:00:00+00:00</updated><id>/jekyll-theme-yat/gatech/2024/01/08/cse6250-2-course-overview</id><content type="html" xml:base="/jekyll-theme-yat/gatech/2024/01/08/cse6250-2-course-overview.html"><![CDATA[<h2 id="big-picture">Big Picture</h2>

<p>Systems / Algorithms / Healthcare Applications</p>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/cddf8f6a-611a-4504-b8a8-e646294439b3" alt="cse6250_02_01" /></p>

<h2 id="healthcare-applications">Healthcare Applications</h2>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/b5441494-98bf-4541-a9a7-9e780c8e56db" alt="cse6250_02_02" /></p>

<ul>
  <li>Predictive modeling (ex. using historical data to build a model to predict a future outcome)</li>
  <li>Computational Phenotyping (ex. turning electronic health records into meaningful clinical concepts)</li>
  <li>Patient similarity (ex. use data to identify groups of patients sharing similar characteristics)</li>
</ul>

<h3 id="predictive-modeling">Predictive Modeling</h3>

<p>Q.</p>

<p>Try to estimate what percentage of people with Epilepsy in the U.S. responded to treatment?</p>

<ul>
  <li>A: ~2 years</li>
  <li>B: 2~5 year</li>
  <li>C: 5~ years</li>
</ul>

<p>A.</p>

<p>32% / 24% / 44%</p>

<p>Early detection for each group B, C will improve the opportunities of treatment.</p>

<h4 id="challenges">Challenges</h4>

<p>What makes predictive modeling difficult?</p>

<ul>
  <li>So much data</li>
</ul>

<p>We have millions of patients, and we want to analyze their diagnosis information, medication information, and so on. So all these data combined together, create a big challenge.</p>

<ul>
  <li>So many models</li>
</ul>

<p>There are so many models to be built. Predictive modeling is not a single algorithm, but a sequence of computational tasks. But every step in pipeline has many different options. All of those combined give us many pipelines to be evaluated and compared.</p>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/8568b415-6832-418e-b1d6-6242822df86f" alt="cse6250_02_03" /></p>

<h3 id="computational-phenotyping">Computational Phenotyping</h3>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/dec6a355-794c-4e58-aa22-e46366dcc22f" alt="cse6250_02_04" /></p>

<p>The input of computation phenotyping is the raw patient data that consists of many different sources.</p>

<ul>
  <li>Demographic information</li>
  <li>Diagnosis</li>
  <li>Medication</li>
  <li>Procedure</li>
  <li>Lab tests</li>
  <li>Clinical notes</li>
</ul>

<p>Computation phenotyping is the process of turning the raw patient data into medical concepts or phenotypes.</p>

<p>Q.</p>

<p>In order to extract phenotypes from raw data, what are some of the ‘waste products’ we should deal with?</p>

<p>A.</p>

<ul>
  <li>Missing data</li>
  <li>Duplicates</li>
  <li>Irrelevant</li>
  <li>Redundant</li>
</ul>

<h4 id="phenotyping-algorithm">Phenotyping Algorithm</h4>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/c01f9875-9267-4905-a42f-c15fecc7d397" alt="cse6250_02_05" /></p>

<p>Let’s take an example of phenotyping algorithm for ‘Type 2 Diabetes’.</p>

<p>The Input is, EHR (electronic health records of a patient)</p>

<p>Q.</p>

<p>Can we just ask whether patient have Type 2 Diabetes diagnoses present in the data?</p>

<p>A.</p>

<p>=&gt; NO.</p>

<p>The electronic health records are very un-reliable (above reasons). Therefore it is not sufficient just checking one source of information.</p>

<h3 id="patient-similarity">Patient Similarity</h3>

<p>Q.</p>

<p>Which of the following types of reasoning do doctors engage most often?</p>

<ul>
  <li>Flowchart reasoning</li>
  <li>Instinct and intuition</li>
  <li>Comparison to past individual patients</li>
</ul>

<p>A.</p>

<p>=&gt; Comparison to past individual patients (or case-based reasoning)</p>

<p>Based on our anecdotal experiences, doctor often compared the current patient to the old patient they have seen.</p>

<h4 id="what-is-patient-similarity">What is Patient Similarity?</h4>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/2f2361b1-b32b-4d40-a666-c757f309d359" alt="cse6250_02_06" /></p>

<p>Simulating the doctor’ case-based reasoning with computer algorithms.</p>

<p>When the patient comes in, the doctor does some examination on the patient. Then based on that information, we can do a similarity search through the database. Find those potentially similar patients, then doctor can provide some supervision on that result to find those truly similar patients to the specific clinical context. Then we can group those patients, based on what treatment they are taking, and look at what outcome they are getting. Then recommend the treatment with the best outcome to the current patient.</p>

<h2 id="algorithms">Algorithms</h2>

<ul>
  <li>Classification</li>
  <li>Clustering</li>
  <li>Dimensionality reduction (X -&gt; X’)</li>
  <li>Graph analysis (connect patients to a set of disease they have, then learn what are the most important patients and diseases in the network and also how do they relate?)</li>
</ul>

<h2 id="systems">Systems</h2>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/2999aea9-45b1-4b16-9b2d-fee7effe9478" alt="cse6250_02_07" />
<img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/fd8b4f0c-39b3-4695-a780-9d97118bb4bd" alt="cse6250_02_08" /></p>

<ul>
  <li>Hadoop (distributed disk based big data system)
    <ul>
      <li>Hadoop Pig, Hive, HBase</li>
      <li>MapReduce, HDFS</li>
      <li>Core</li>
    </ul>
  </li>
  <li>Spark (distributed in-memory based big data system)
    <ul>
      <li>Spark SQL</li>
      <li>Spark streaming</li>
      <li>MLLib (for distributed large scale ML)</li>
      <li>GraphX</li>
    </ul>
  </li>
</ul>

<h2 id="summary">Summary</h2>

<p>With Systems / Algorithms / Healthcare Applications all together.</p>

<p>For example, we build ‘a scalable classifier using logistic regression’ on ‘Hadoop’ for ‘predicting heart failure’</p>]]></content><author><name>taehyeok-jang</name></author><category term="gatech" /><category term="gatech" /><category term="big-data" /><category term="machine-learning" /><summary type="html"><![CDATA[Big Picture]]></summary></entry><entry><title type="html">CSE6250 Big Data for Healthcare - 3. Predictive Modeling</title><link href="/jekyll-theme-yat/gatech/2024/01/08/cse6250-3-predictive-modeling.html" rel="alternate" type="text/html" title="CSE6250 Big Data for Healthcare - 3. Predictive Modeling" /><published>2024-01-08T00:00:00+00:00</published><updated>2024-01-08T00:00:00+00:00</updated><id>/jekyll-theme-yat/gatech/2024/01/08/cse6250-3-predictive-modeling</id><content type="html" xml:base="/jekyll-theme-yat/gatech/2024/01/08/cse6250-3-predictive-modeling.html"><![CDATA[<h2 id="intro">Intro</h2>

<p>A process of modeling historical data for predicting future events.</p>

<p>For example, use electronic health records to build a model of heart failures. Therefore, the key question is, how do we develop such a predictive model quickly?</p>

<p>Ex. EHR</p>

<p>From 2010, EHR become a major data sources of clinical predictive modeling research.</p>

<h2 id="predictive-modeling-pipeline">Predictive Modeling Pipeline</h2>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/690fa3f6-15c6-4b62-9658-d37951e3c7bd" alt="cse6250_03_01" /></p>

<ol>
  <li>Prediction target</li>
  <li>Cohort construction</li>
  <li>Feature construction</li>
  <li>Feature selection</li>
  <li>Predictive model</li>
  <li>Performance evaluation</li>
</ol>

<p>AND iterate!</p>

<h2 id="prediction-target">Prediction Target</h2>

<h3 id="motivations-for-early-detection-of-heart-failure">Motivations for Early Detection of Heart Failure</h3>

<p>Q.</p>

<p>How many new cases of heart failure occur each year in the US?</p>

<p>A.</p>

<p>550K</p>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/b1a9059d-d113-4dc8-9a35-fe6a37b80e3f" alt="cse6250_03_02" /></p>

<p>Complexity.</p>

<p>There is no widely-accepted characterizations and definition of heart failure, probably because the complexity of the syndrome.</p>

<p>It has many potential ideologies, diverse clinical features, and numerous clinical subsets.</p>

<h2 id="cohort-construction">Cohort Construction</h2>

<p>How do we define the study population?</p>

<p>There are two different axes to be considered.</p>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/17ed706f-895b-408d-87df-d04e6c531a82" alt="cse6250_03_03" />
<img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/5b9a207e-b9d5-44f3-ad4d-820bc063ab49" alt="cse6250_03_04" /></p>

<h3 id="prospective-vs-retrospective">Prospective vs Retrospective</h3>

<ul>
  <li>Prospective</li>
</ul>

<p>Identify the cohort of patients,</p>

<p>Decide what information to collect and how to collect them,</p>

<p>Start the data collection (from scratch)</p>

<p>More expensive, takes a longer time</p>

<ul>
  <li>Retrospective</li>
</ul>

<p>Identify the cohort of patients from existing data,</p>

<p>Retrieve all the data about the cohort.</p>

<p>More noises, common on large dataset</p>

<h3 id="study-methods">Study methods</h3>

<ul>
  <li>Cohort Study</li>
</ul>

<p>We identify all the patients who are exposed to the risk and the matching criteria’s are not involved.</p>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/9245e167-f8ce-4c39-9c90-3ecc734d54aa" alt="cse6250_03_05" /></p>

<ul>
  <li>Case-control study</li>
</ul>

<p>We first identify the cases, then try to match them to a set of control patients.</p>

<p>KEY: to develop match~</p>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/7d0e8c11-5c9c-4e50-891b-00e285bd253c" alt="cse6250_03_06" />
<img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/47115a8e-fc71-4bea-9c30-8b4060b216f5" alt="cse6250_03_07" /></p>

<h2 id="feature-construction">Feature Construction</h2>

<p>The goal of feature construction is to construct all potentially relevant features about patients in order to predict the target outcome.</p>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/e808744f-979c-4dbc-8669-75cf29bfdacf" alt="cse6250_03_08" /></p>

<ul>
  <li>Observation window</li>
</ul>

<p>We use all the patient information happening this observation window to construct features.</p>

<ul>
  <li>Index date</li>
</ul>

<p>A date to use to learn predictive model to make a prediction about the target outcome.</p>

<ul>
  <li>Prediction window</li>
  <li>Diagnosis date</li>
</ul>

<p>the data that the target outcome happens.</p>

<p>The case patient is diagnosed with heart failure on this date. The control patient does not have. So in theory, we can use any days from control patient as the diagnosis date. But commonly we use the same data of the matching case patient for the corresponding control.</p>

<h3 id="methods-to-construct-features">Methods to Construct Features</h3>

<ul>
  <li>Count the number of times an event happens</li>
</ul>

<p>If type 2 diabetes codes happens three times during the observation window, the corresponding feature for type 2 diabetes = 3.</p>

<ul>
  <li>Take average of the event value.</li>
</ul>

<p>If patient has two HBA1C measures during observation window, we can take the average of this two measurement as a feature for HBA1C.</p>

<p>The length of prediction window and observation window are two important parameters that goin to impact the model performance.</p>

<p>Q.</p>

<p>Which one of these timelines is <strong>the easiest for</strong> modeling?</p>

<p>A.</p>

<p>Large observation window &amp; Small prediction window</p>

<p>It is often easier to predict event in the near future (small prediction window). And large observation mean more information to be used to construct features, which is often better since w can model patient better with more data.</p>

<p>Q.</p>

<p>Which one of these timelines is <strong>the most useful</strong> model?</p>

<p>A.</p>

<p>Small observation window &amp; Large prediction window.</p>

<p>It is a quite ideal. In this ideal situation, if we can construct a good model, we want to predict far into future. (Even without much data about patients). However, this setting is often difficult to model, therefore unrealistic.</p>

<h3 id="prediction-performance-on-different-prediction-window">Prediction performance on Different Prediction window</h3>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/09146b61-b073-4549-9e37-adf3339058ae" alt="cse6250_03_09" /></p>

<p>As another example, we can find that the accuracy of the model drops as we increase the prediction window, because it is easier to predict the near future than things happened far into the future.</p>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/97ca4c2c-fe16-4349-903c-395fa793ded7" alt="cse6250_03_10" /></p>

<p>Q.</p>

<p>Which of these options is the most desirable prediction curve?</p>

<p>A.</p>

<p>We can predict accurately for fairly long period of time. While the performance of the other models drop fairly quickly as the prediction window increases.</p>

<h3 id="prediction-performance-on-different-observation-windows">Prediction performance on different observation windows.</h3>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/cb7cec6d-7a94-4a98-9aa5-ae3603b27d6b" alt="cse6250_03_11" /></p>

<p>Typically, as the observation window increases, the performance improves, because we know more about the patients.</p>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/37416af1-b0ab-4f5a-b050-ed3384a560c6" alt="cse6250_03_12" /></p>

<p>Q.</p>

<p>What is the optimal observation window?</p>

<p>A.</p>

<p>630 days</p>

<p>The model performance plateaued after 630 days. It indicates a diminishing return if we go further beyond that point.</p>

<p>We may choose 900 days, but it’s a trade-off between how long is the observation window and how many patents have that much data. So if we choose 900 days, for patients who do not have enough data up to 900 days, they will be excluded from the study.</p>

<h2 id="feature-selection">Feature Selection</h2>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/84967f12-b985-461a-aef9-8428b727a217" alt="cse6250_03_13" /></p>

<p>The goal of feature selection is to find the truly predictive features to be included in the model.</p>

<p>We can construct features using patients’ even sequences from raw data in the observation window. We can construct features from all of these events. However, not all events are relevant for predicting a specific target.</p>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/7ce0097b-2cfc-4f4d-b9ca-d1331aca9b55" alt="cse6250_03_14" />
<img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/341dcd37-1f7f-43fb-a05a-1dd8d30b39bb" alt="cse6250_03_15" /></p>

<p>In reality, the patient chart is quite complex over 20,000 features from a typical EHR dataset. Not all of this are relevant for predicting a target.</p>

<p>For example, if we want to predict a heart failure, those yellow features are relevant. However, for a different condition such as diabetes, maybe those purple features are relevant.</p>

<h2 id="predictive-model">Predictive Model</h2>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/1c821c6e-1853-4f1b-be39-1c7f6241d075" alt="cse6250_03_16" /></p>

<p>Predictive model is the function that maps the input features of the patient to the output target.</p>

<p>For example, if we know a patient’s past diagnosis, medication, and lab result, if we also know this function, then we can assess how likely the patient will have heart failure.</p>

<h2 id="performance-evaluation">Performance Evaluation</h2>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/7f0598ca-386a-402e-b9b6-5385fcb296eb" alt="cse6250_03_17" /></p>

<p>Evaluation of predictive models is one of the most crucial steps in the pipeline.</p>

<ul>
  <li>Training error is NOT very useful.</li>
</ul>

<p>We can very easily overfit the training data by using complex model which do not generalize well to future samples.</p>

<ul>
  <li>Testing error is the KEY metric.</li>
</ul>

<p>It is a better approximation of the true perfjoamcne of the model on future samples.</p>

<h3 id="cross-validation">Cross Validation</h3>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/ff973a96-55f2-4a08-8dca-8cfe19b291b6" alt="cse6250_03_18" /></p>

<p>The main idea behind the cross-validation, is to iteratively split a dataset into training and validation set. We build the model on the training set, and test the model on the validation set, but do this iteratively many times. Finally the performance matrix are aggregated across these iterations.</p>

<h4 id="3-common-methods-for-cross-validation">3 common methods for Cross Validation</h4>

<ul>
  <li>Leave-1-out CV</li>
</ul>

<p>Take one example at a time as our validation set, use the remaining set as the training set.</p>

<ul>
  <li>K-fold CV</li>
</ul>

<p>Split the entire dataset into K-folds, we iteratively choose each fold as a validation set, use the remaining folds as a training set to build another model.</p>

<p>The final performance is the average over these K different models.</p>

<ul>
  <li>Randomized CV</li>
</ul>

<p>Randomly split the dataset into training and testing.</p>

<p>Randomized vs K-fold?</p>

<p>Adv.</p>

<p>The proportion of the training and validation set does not depend on the number of folds.</p>

<p>Disadv.</p>

<p>Some observation may never be selected into the validation set because there’s randomization process, whereas some other examples may be selected more than ones.</p>]]></content><author><name>taehyeok-jang</name></author><category term="gatech" /><category term="gatech" /><category term="big-data" /><category term="machine-learning" /><summary type="html"><![CDATA[Intro]]></summary></entry><entry><title type="html">Clickhouse - Materialized View</title><link href="/jekyll-theme-yat/database/2023/04/15/clickhouse-materialized-views.html" rel="alternate" type="text/html" title="Clickhouse - Materialized View" /><published>2023-04-15T00:00:00+00:00</published><updated>2023-04-15T00:00:00+00:00</updated><id>/jekyll-theme-yat/database/2023/04/15/clickhouse-materialized-views</id><content type="html" xml:base="/jekyll-theme-yat/database/2023/04/15/clickhouse-materialized-views.html"><![CDATA[<p><a href="https://taehyeok-jang.github.io/database/2023/04/01/clickhouse-internal.html">ClickHouse 1편 - Clickhouse Internal</a></p>

<p>이전 글에서 알아보았듯이 ClickHouse는 distributed column-oriented SQL data warehouse로서 anlaytical purpose을 위한 저장, 연산 등 핵심적인 역할을 수행합니다. 그러나 여전히 ClickHouse는 materialized view라는 도구를 통해서 쿼리 성능을 향상시키고 데이터 관리성을 높일 수 있는 잠재력을 가지고 있습니다.</p>

<p>이번 글에서는 ClickHouse의 materialized view에 대해서 알아보고 몇가지 활용 사례에 대해서도 소개하겠습니다.</p>

<h2 id="what-is-a-materialized-view">What is a Materialized View?</h2>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/77eefcbc-cb92-44d2-bab0-fbfc43a7f9b8" alt="materialized_view_5a321dc56d" style="zoom: 33%;" /></p>

<p>Materialized view는 ClickHouse에서 지원하는 view의 한 종류입니다. 일반적인 view는 단순한 SELECT 쿼리문 라고 할 수 있습니다. 하지만 이러한 view는 아무런 데이터도 저장하지 않는 반면, materialized view는 실제로 별도의 테이블에 SELECT쿼리의 결과로 얻어지는 데이터들을 저장합니다. materialized view는 source table과 연결되어, 새로운 데이터가 source table에 insert 되었을 때 동일한 row가 materialized view에도 저장됩니다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Type</th>
      <th style="text-align: left">**Description**</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Normal</td>
      <td style="text-align: left">Nothing more than a **saved query**. When reading from a view, this saved query is used as a subquery in the **FROM** clause.</td>
    </tr>
    <tr>
      <td style="text-align: left">Materialized</td>
      <td style="text-align: left">**Stores the results** of the corresponding **SELECT** query.</td>
    </tr>
  </tbody>
</table>

<p>ClickHouse에서는 materialized view를 통해서 다음과 같은 작업을 할 수 있습니다.</p>

<ul>
  <li>compute aggregates</li>
  <li>read data from Kafka</li>
  <li>implement last point queries</li>
  <li>reorganize table primary indexes and sort order</li>
</ul>

<p>이러한 기능들을 단순히 제공하는 것 이상으로 ClickHouse는 materialized view를 large dataset을 대상으로 여러 node들에서 수행될 수 있도록 하는 확장성을 제공합니다.</p>

<h2 id="using-materialized-view-computing-sums">Using Materialized View: Computing Sums</h2>

<p>분석을 위한 대표적인 연산인 aggregation 연산을 materialized view을 통해서 수행해보겠습니다.</p>

<p>테스트 dataset은 <a href="https://clickhouse.com/docs/en/getting-started/example-datasets/wikistat">wikistat</a> 을 활용하였으며 데이터의 크기는 총 1B row입니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CREATE TABLE wikistat
(
    `time` DateTime CODEC(Delta(4), ZSTD(1)),
    `project` LowCardinality(String),
    `subproject` LowCardinality(String),
    `path` String,
    `hits` UInt64
)
ENGINE = MergeTree
ORDER BY (path, time);


INSERT INTO wikistat SELECT *
FROM s3('https://ClickHouse-public-datasets.s3.amazonaws.com/wikistat/partitioned/wikistat*.native.zst') LIMIT 1e9

...
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Query id: 66b182ad-2422-4319-854f-50056aebba96
← Progress: 1.00 billion rows, 1.99 GB (2.27 million rows/s., 4.52 MB/s.)

Elapsed: 499.015 sec. Processed 1.00 billion rows, 1.99 GB (2.00 million rows/s., 3.99 MB/s.)
Peak memory usage: 362.64 MiB.
</code></pre></div></div>

<p>특정 날짜에 project 별로 방문자수 별 랭킹을 확인하는 쿼리를 실행해보겠습니다.</p>

<p>ClickHouse Cloud 기준으로 약 15초가 걸렸습니다. column-oriented 저장소로서 aggregation 연산에 특화되어 있음에도 불구하고 대량의 데이터를 연산하는 탓에 상당한 시간이 소요되었습니다. 만약 전체 연산 흐름에서 이 쿼리를 빈번하게 실행한다면 병목이 발생할 것입니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SELECT
    project,
    sum(hits) AS h
FROM wikistat
WHERE date(time) = '2015-05-01'
GROUP BY project
ORDER BY h DESC
LIMIT 10

Query id: bd1509b8-27ac-4830-a569-1b8edf3b1cdc

┌─project─┬────────h─┐
│ en      │ 32651529 │
│ es      │  3843097 │
│ de      │  3581099 │
│ fr      │  2970210 │
│ it      │  1719488 │
│ ja      │  1387605 │
│ pt      │  1173172 │
│ commons │   962568 │
│ zh      │   931435 │
│ tr      │   735252 │
└─────────┴──────────┘

10 rows in set. Elapsed: 14.869 sec. Processed 972.80 million rows, 10.53 GB (65.43 million rows/s., 708.05 MB/s.)
</code></pre></div></div>

<p>이제 materialized view를 활용해서 쿼리를 수행하겠습니다.</p>

<p>materialized view는 원하는 만큼 생성할 수 있지만 materialized view의 개수가 늘어날수록 storage load가 발생할 수 있기 때문에 테이블 별로 10개 이하의 materialized view를 권장한다고 합니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CREATE TABLE wikistat_top_projects
(
    `date` Date,
    `project` LowCardinality(String),
    `hits` UInt32
)
ENGINE = SummingMergeTree
ORDER BY (date, project);

Ok.

CREATE MATERIALIZED VIEW wikistat_top_projects_mv TO wikistat_top_projects AS
SELECT
    date(time) AS date,
    project,
    sum(hits) AS hits
FROM wikistat
GROUP BY
    date,
    project;
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">wikistat_top_projects</code> 는 materialized view의 결과를 저장할 대상 테이블입니다.</li>
  <li><code class="language-plaintext highlighter-rouge">wikistat_top_projects_mv</code> materialized view 그 자체 (trigger)의 이름입니다.</li>
  <li>대상 테이블인 <code class="language-plaintext highlighter-rouge">wikistat_top_projects</code>의 테이블 엔진으로는 <a href="https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/summingmergetree/">SummingMergeTree</a> 를 사용하였습니다. project 별 hits 값을 sum하는 연산을 수행하기 때문입니다.</li>
  <li><code class="language-plaintext highlighter-rouge">AS</code> 이후가 materialized view의 생성을 위한 쿼리문입니다.</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">wikistat_top_projects</code> 테이블을 대상으로 쿼리를 수행하니 0.023초가 소요되었습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SELECT
    project,
    sum(hits) hits
FROM wikistat_top_projects
WHERE date = '2015-05-01'
GROUP BY project
ORDER BY hits DESC
LIMIT 10

┌─project─┬─────hits─┐
│ en      │ 34521803 │
│ es      │  4491590 │
│ de      │  4490097 │
│ fr      │  3390573 │
│ it      │  2015989 │
│ ja      │  1379148 │
│ pt      │  1259443 │
│ tr      │  1254182 │
│ zh      │   988780 │
│ pl      │   985607 │
└─────────┴──────────┘

10 rows in set. Elapsed: 0.023 sec. Processed 9.50 thousand rows, 76.00 KB (416.20 thousand rows/s., 3.33 MB/s.)
Peak memory usage: 18.73 KiB.
</code></pre></div></div>

<h3 id="sync-in-materialized-views">Sync in Materialized Views</h3>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/e766f386-6a11-48d5-9a97-0e5b672ce9e2" alt="updating_materialized_view_b90a9ac7cb" /></p>

<p>materialized view를 사용하는 데 있어서 장점은 source table의 변경사항이 materialized view에도 실시간으로 반영이 된다는 점입니다. 이는 쿼리 최적화를 위해 materialized view를 더욱 유연하게 활용할 수 있다는 의미입니다.</p>

<p>예제를 통해 살펴보겠습니다. 테이블은 device별 floating point 값을 저장합니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CREATE TABLE counter (
  when DateTime DEFAULT now(),
  device UInt32,
  value Float32
) ENGINE=MergeTree
PARTITION BY toYYYYMM(when)
ORDER BY (device, when)
</code></pre></div></div>

<p>1B row개의 mock data를 입력하고 전체 기간에 대한 aggregation 쿼리를 수행해보겠습니다. 약 2.7초가 소요되는 것을 확인할 수 있습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>INSERT INTO counter
  SELECT
    toDateTime('2015-01-01 00:00:00') + toInt64(number/10) AS when,
    (number % 10) + 1 AS device,
    (device * 3) +  (number/10000) + (rand() % 53) * 0.1 AS value
  FROM system.numbers LIMIT 1000000
  
SELECT
    device,
    count(*) AS count,
    max(value) AS max,
    min(value) AS min,
    avg(value) AS avg
FROM counter
GROUP BY device
ORDER BY device ASC
. . .
10 rows in set. Elapsed: 2.709 sec. Processed 1.00 billion rows, 8.00 GB (369.09 million rows/s., 2.95 GB/s.)
</code></pre></div></div>

<p>이제 materialized view를 생성하고 source table에 데이터를 insert하여 materialized view의 대상 테이블로 변경사항이 trigger 될 수 있도록 하겠습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CREATE TABLE counter_daily (
  day DateTime,
  device UInt32,
  count UInt64,
  max_value_state AggregateFunction(max, Float32),
  min_value_state AggregateFunction(min, Float32),
  avg_value_state AggregateFunction(avg, Float32)
)
ENGINE = SummingMergeTree()
PARTITION BY tuple()
ORDER BY (device, day)

CREATE MATERIALIZED VIEW counter_daily_mv
TO counter_daily
AS SELECT
    toStartOfDay(when) as day,
    device,
    count(*) as count,
    maxState(value) AS max_value_state,
    minState(value) AS min_value_state,
    avgState(value) AS avg_value_state
FROM counter
WHERE when &gt;= toDate('2019-01-01 00:00:00')
GROUP BY device, day
ORDER BY device, day
</code></pre></div></div>

<p>source table에 데이터를 insert 합니다. 변경사항은 실시간으로 materialized view를 통해 udpate 됩니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>INSERT INTO counter
  SELECT
    toDateTime('2015-01-01 00:00:00') + toInt64(number/10) AS when,
    (number % 10) + 1 AS device,
    (device * 3) +  (number/10000) + (rand() % 53) * 0.1 AS value
  FROM system.numbers LIMIT 1000000
</code></pre></div></div>

<p>이제 쿼리를 수행하겠습니다. 0.015초만에 쿼리 수행이 완료되었습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SELECT
  device,
  sum(count) AS count,
  maxMerge(max_value_state) AS max,
  minMerge(min_value_state) AS min,
  avgMerge(avg_value_state) AS avg
FROM counter_daily
GROUP BY device
ORDER BY device ASC

=&gt; 
Query id: 149617cf-a266-44ad-a193-de9ef50e9956

┌─device─┬───count─┬───────max─┬─────min─┬────────────────avg─┐
│      1 │ 1000000 │  1008.194 │   3.027 │  505.5990479240353 │
│      2 │ 1000000 │ 1011.1521 │  6.0191 │   508.598602163702 │
│      3 │ 1000000 │ 1014.1392 │  9.0012 │ 511.59625628831293 │
│      4 │ 1000000 │ 1017.1973 │ 12.0613 │  514.5981650169449 │
│      5 │ 1000000 │ 1020.1234 │ 15.0224 │  517.5985900957355 │
│      6 │ 1000000 │ 1023.1875 │ 18.0365 │  520.5979234121476 │
│      7 │ 1000000 │ 1026.1846 │ 21.0216 │  523.5997531775646 │
│      8 │ 1000000 │ 1029.1937 │ 24.0947 │  526.6002109527683 │
│      9 │ 1000000 │ 1032.0928 │ 27.0228 │  529.5988765025311 │
│     10 │ 1000000 │  1035.169 │ 30.0979 │  532.6029915245838 │
└────────┴─────────┴───────────┴─────────┴────────────────────┘

10 rows in set. Elapsed: 0.015 sec
</code></pre></div></div>

<h2 id="using-materialized-view-kafka-table-engine">Using Materialized View: Kafka table engine</h2>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/27de1992-18fa-4d09-b19e-0611e5f1cb4b" alt="kafka_01-807249e726cadc9d3be21375df967d42" /></p>

<p>materialized view를 활용해서 ClickHouse의 테이블에 Kafka topic의 data stream을 저장하는 것이 가능합니다.</p>

<p>Kafka table engine은 ClickHouse가 Kafka topic을 직접 읽는 것을 가능하게 합니다. 이때 Kafka table engine에서는 마지막으로 topic을 읽은 위치(offset)를 기억하여 poll을 통해 data stream을 한번씩 읽을 수 있도록 합니다.</p>

<p>Kafka table engine으로 읽은 topic을 저장하기 위해서는 이러한 data stream을 감지하고 테이블에 저장할 수 있는 장치가 필요한데, 바로 materialized view의 trigger 특성을 활용하여 가능합니다. Kafka topic을 Kafka table engine이 읽고, Kafka table engine의 변경사항으로 인해 발생하는 trigger를 materialized view가 감지하여 ClickHouse의 테이블에 저장합니다.</p>

<p>예제를 통해 살펴보겠습니다.</p>

<p>먼저 최종적으로 데이터를 저장할 테이블을 생성합니다. 테이블 엔진은 MergeTree을 사용했습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CREATE TABLE github
(
    file_time DateTime,
    event_type Enum('CommitCommentEvent' = 1, 'CreateEvent' = 2, 'DeleteEvent' = 3, 'ForkEvent' = 4, 'GollumEvent' = 5, 'IssueCommentEvent' = 6, 'IssuesEvent' = 7, 'MemberEvent' = 8, 'PublicEvent' = 9, 'PullRequestEvent' = 10, 'PullRequestReviewCommentEvent' = 11, 'PushEvent' = 12, 'ReleaseEvent' = 13, 'SponsorshipEvent' = 14, 'WatchEvent' = 15, 'GistEvent' = 16, 'FollowEvent' = 17, 'DownloadEvent' = 18, 'PullRequestReviewEvent' = 19, 'ForkApplyEvent' = 20, 'Event' = 21, 'TeamAddEvent' = 22),
    actor_login LowCardinality(String),
    repo_name LowCardinality(String),
    created_at DateTime,
    updated_at DateTime,
    action Enum('none' = 0, 'created' = 1, 'added' = 2, 'edited' = 3, 'deleted' = 4, 'opened' = 5, 'closed' = 6, 'reopened' = 7, 'assigned' = 8, 'unassigned' = 9, 'labeled' = 10, 'unlabeled' = 11, 'review_requested' = 12, 'review_request_removed' = 13, 'synchronize' = 14, 'started' = 15, 'published' = 16, 'update' = 17, 'create' = 18, 'fork' = 19, 'merged' = 20),
    comment_id UInt64,
    path String,
    ref LowCardinality(String),
    ref_type Enum('none' = 0, 'branch' = 1, 'tag' = 2, 'repository' = 3, 'unknown' = 4),
    creator_user_login LowCardinality(String),
    number UInt32,
    title String,
    labels Array(LowCardinality(String)),
    state Enum('none' = 0, 'open' = 1, 'closed' = 2),
    assignee LowCardinality(String),
    assignees Array(LowCardinality(String)),
    closed_at DateTime,
    merged_at DateTime,
    merge_commit_sha String,
    requested_reviewers Array(LowCardinality(String)),
    merged_by LowCardinality(String),
    review_comments UInt32,
    member_login LowCardinality(String)
) ENGINE = MergeTree ORDER BY (event_type, repo_name, created_at)
</code></pre></div></div>

<p>다음으로 Kafka table engine을 생성합니다.</p>

<p>테이블 엔진을 Kafka로 합니다. 스키마가 대상 테이블과 일치하지만 반드시 같을 필요는 없습니다. 필요에 따라 Kafka table engine이나 materialized view에서 transform 과정을 포함시키기도 합니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CREATE TABLE github_queue
(
    file_time DateTime,
    event_type Enum('CommitCommentEvent' = 1, 'CreateEvent' = 2, 'DeleteEvent' = 3, 'ForkEvent' = 4, 'GollumEvent' = 5, 'IssueCommentEvent' = 6, 'IssuesEvent' = 7, 'MemberEvent' = 8, 'PublicEvent' = 9, 'PullRequestEvent' = 10, 'PullRequestReviewCommentEvent' = 11, 'PushEvent' = 12, 'ReleaseEvent' = 13, 'SponsorshipEvent' = 14, 'WatchEvent' = 15, 'GistEvent' = 16, 'FollowEvent' = 17, 'DownloadEvent' = 18, 'PullRequestReviewEvent' = 19, 'ForkApplyEvent' = 20, 'Event' = 21, 'TeamAddEvent' = 22),
    actor_login LowCardinality(String),
    repo_name LowCardinality(String),
    created_at DateTime,
    updated_at DateTime,
    action Enum('none' = 0, 'created' = 1, 'added' = 2, 'edited' = 3, 'deleted' = 4, 'opened' = 5, 'closed' = 6, 'reopened' = 7, 'assigned' = 8, 'unassigned' = 9, 'labeled' = 10, 'unlabeled' = 11, 'review_requested' = 12, 'review_request_removed' = 13, 'synchronize' = 14, 'started' = 15, 'published' = 16, 'update' = 17, 'create' = 18, 'fork' = 19, 'merged' = 20),
    comment_id UInt64,
    path String,
    ref LowCardinality(String),
    ref_type Enum('none' = 0, 'branch' = 1, 'tag' = 2, 'repository' = 3, 'unknown' = 4),
    creator_user_login LowCardinality(String),
    number UInt32,
    title String,
    labels Array(LowCardinality(String)),
    state Enum('none' = 0, 'open' = 1, 'closed' = 2),
    assignee LowCardinality(String),
    assignees Array(LowCardinality(String)),
    closed_at DateTime,
    merged_at DateTime,
    merge_commit_sha String,
    requested_reviewers Array(LowCardinality(String)),
    merged_by LowCardinality(String),
    review_comments UInt32,
    member_login LowCardinality(String)
)
   ENGINE = Kafka('kafka_host:9092', 'github', 'clickhouse',
            'JSONEachRow') settings kafka_thread_per_consumer = 0, kafka_num_consumers = 1;
</code></pre></div></div>

<p>materialized view를 생성합니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CREATE MATERIALIZED VIEW github_mv TO github AS
SELECT *
FROM github_queue;
</code></pre></div></div>

<p>이제 Kafka topic으로 메시지를 publish합니다. 예제에서는 파일 스트림에 kafkacat을 연결하였습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cat github_all_columns.ndjson | 
kcat -P \
  -b 'kafka_host:9092' \
  -t github
  -X security.protocol=sasl_ssl \
  -X sasl.mechanisms=PLAIN \
  -X sasl.username=&lt;username&gt;  \
  -X sasl.password=&lt;password&gt; \
</code></pre></div></div>

<p>결과를 확인합니다. 최종 테이블에 200K의 row가 잘 insert 된 것을 확인할 수 있습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SELECT count() FROM github;
=&gt; 
┌─count()─┐
│  200000 │
└─────────┘
</code></pre></div></div>

<h2 id="references">References</h2>

<ul>
  <li>ClickHouse
    <ul>
      <li><a href="https://clickhouse.com/blog/using-materialized-views-in-clickhouse">https://clickhouse.com/blog/using-materialized-views-in-clickhouse</a></li>
      <li><a href="https://clickhouse.com/docs/en/sql-reference/statements/create/view#materialized-view">https://clickhouse.com/docs/en/sql-reference/statements/create/view#materialized-view</a></li>
      <li><a href="https://learn.clickhouse.com/learner_module/show/1043451?lesson_id=5684730&amp;section_id=48330606">https://learn.clickhouse.com/learner_module/show/1043451?lesson_id=5684730&amp;section_id=48330606</a></li>
      <li><a href="https://clickhouse.com/docs/knowledgebase/are_materialized_views_inserted_asynchronously">https://clickhouse.com/docs/knowledgebase/are_materialized_views_inserted_asynchronously</a></li>
      <li><a href="https://clickhouse.com/docs/en/integrations/kafka/kafka-table-engine">https://clickhouse.com/docs/en/integrations/kafka/kafka-table-engine</a></li>
    </ul>
  </li>
  <li>Affinity
    <ul>
      <li><a href="https://altinity.com/blog/clickhouse-materialized-views-illuminated-part-1">https://altinity.com/blog/clickhouse-materialized-views-illuminated-part-1</a></li>
      <li><a href="https://altinity.com/blog/clickhouse-materialized-views-illuminated-part-2">https://altinity.com/blog/clickhouse-materialized-views-illuminated-part-2</a></li>
    </ul>
  </li>
</ul>]]></content><author><name>taehyeok-jang</name></author><category term="database" /><category term="clickhouse" /><category term="materialized-view" /><summary type="html"><![CDATA[ClickHouse 1편 - Clickhouse Internal]]></summary></entry><entry><title type="html">Clickhouse - Internal</title><link href="/jekyll-theme-yat/database/2023/04/01/clickhouse-internal.html" rel="alternate" type="text/html" title="Clickhouse - Internal" /><published>2023-04-01T00:00:00+00:00</published><updated>2023-04-01T00:00:00+00:00</updated><id>/jekyll-theme-yat/database/2023/04/01/clickhouse-internal</id><content type="html" xml:base="/jekyll-theme-yat/database/2023/04/01/clickhouse-internal.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>ClickHouse is an open source column-oriented distributed OLAP database, also refered as a SQL data warehouse.</p>

<ul>
  <li>Open source
    <ul>
      <li>Since 2009, 22K+ GitHub stars, 900+ contributors, 300+ releases</li>
    </ul>
  </li>
  <li>Column-oriented
    <ul>
      <li>Best for aggregations</li>
      <li>Files per column</li>
      <li>Sorting and indexing</li>
      <li>Background merges</li>
    </ul>
  </li>
  <li>Distributed
    <ul>
      <li>Replication</li>
      <li>Sharding</li>
      <li>Multi-master</li>
      <li>Cross-region</li>
    </ul>
  </li>
  <li>OLAP
    <ul>
      <li>Analytics use cases</li>
      <li>Aggregations</li>
      <li>Visualizations</li>
      <li>Mostly immutable data</li>
    </ul>
  </li>
</ul>

<h2 id="key-features">Key Features</h2>

<ul>
  <li>ANSI-compatible SQL</li>
</ul>

<p>Most SQL-compatible UIs, editors, applications, frameworks will just work!</p>

<ul>
  <li>Lots of writes</li>
</ul>

<p>Up to several million writes per second - in fact, we’ll see 2.5M writes per second later!</p>

<ul>
  <li>Distributed</li>
</ul>

<p>Replicated and sharded, largest known cluster consists of about 4,000 servers.</p>

<ul>
  <li>Highly efficient storage</li>
</ul>

<p>Lots of encoding and compression options - we’ll see 20x compression later.</p>

<ul>
  <li>Very fast queries</li>
</ul>

<p>Scan and process even billions of rows per second and use vectorized query execution.</p>

<ul>
  <li>Joins and lookups</li>
</ul>

<p>Allows separating fact and dimension tables in a star schema.</p>

<h2 id="architecture">Architecture</h2>

<p><img width="1322" alt="clickhouse-getting_started_with_03" src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/8aa03a01-9a69-494c-b4ba-41bc2f1db4ad" /></p>

<ul>
  <li>Distributed mode</li>
</ul>

<p>ClickHouse supports both shard and replication across a distributed cluster.</p>

<ul>
  <li>Coordination</li>
</ul>

<p>ClickHouse Keeper provides the coordination system for data replication and distributed DDL queries execution. ClickHouse Keeper is compatible with ZooKeeper.
ClickHouse Keeper uses the RAFT algorithm implementation, similar to Kafka broker’s RAFT implementation.</p>

<p>&lt;-&gt;
Unlike ClickHouse, Apache HBase and Google BigTable has a master server that coordinates a whole cluster.</p>

<p>The master is responsible for,</p>
<ul>
  <li>assigning tablets to tablet servers,</li>
  <li>detecting the addition and expiration of tablet servers,</li>
  <li>balancing tablet-server load,</li>
  <li>and garbage collection of files in GFS.</li>
  <li>In addition, it handles schema changes such as table and column family creations.</li>
</ul>

<p><img width="1322" alt="clickhouse-getting_started_with_02" src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/026da380-d2a3-44e6-8395-e86be03bef04" /></p>

<h2 id="mergetree">MergeTree</h2>
<h3 id="table-engine">Table Engine</h3>
<p>https://www.alibabacloud.com/blog/selecting-a-clickhouse-table-engine_597726?spm=a2c65.11461447.0.0.530e484b9y6QtJ</p>

<p>ClickHouse provides about 28 table engines for different purposes. For example,</p>
<ul>
  <li>Log family for small table data analysis, however</li>
  <li>MergeTree family for big-volume data analysis</li>
  <li>Integration for external data integration.</li>
</ul>

<p>However Log, Special, and Integration are mainly used for special purposes in relatively limited scenarios. <strong>MergeTree family is the officially recommended storage engine,</strong> which supports almost all ClickHouse core functions.</p>

<p>There are a wide range of MergeTree table engines including MergeTree, ReplacingMergeTree, CollapsingMergeTree, VersionedCollapsingMergeTree, SummingMergeTree, and AggregatingMergeTree,… (some supports data aggregation and deduplication)</p>

<ul>
  <li>Example</li>
</ul>

<p>We adopts ‘ReplacingMergeTree’ for internal data pipeline project.</p>

<ul>
  <li>Does MergeTree remove some rows by compacting process?</li>
</ul>

<p>Some MergeTree families such as ReplacingMergeTree, SummingMergeTree, AggregatingMergeTree does replace / sum / aggregate rows.</p>

<h3 id="mergetree-as-lsm-tree">MergeTree as LSM-Tree</h3>

<ul>
  <li>LSM-Tree?</li>
</ul>

<p>=&gt; read this article! <a href="https://taehyeok-jang.github.io/database/2020/09/27/algorithms-behind-modern-storage-systems.html#h-introduction">(Paper Review) Algorithms Behind Modern Storage Systems</a></p>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/af82c9e1-49c9-47db-ab42-256fb2b5efca" alt="LSM_Tree_(1)" /></p>

<p>MergeTree has the same core idea as LSM-Tree, to solve the performance problem of random disk writing.
MergeTree storage structure sorts the written data first and then stores it. Orderly data storage has two core advantages:</p>

<ul>
  <li>When column-store files are compressed by blocks, the column values in the sort key are continuous or repeated, so t<strong>he column-store blocks can be compressed at an excellent data compression ratio.</strong></li>
  <li>Orderly storage <strong>is an index structure that helps accelerate queries.</strong> The approximate position interval where the target rows are can be located quickly based on the equivalence condition or range condition of the columns in the sort key.</li>
</ul>

<h3 id="mergetree-internal">MergeTree Internal</h3>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">user_action_log</span> <span class="p">(</span>
  <span class="nv">`time`</span> <span class="nb">DateTime</span> <span class="k">DEFAULT</span> <span class="k">CAST</span><span class="p">(</span><span class="s1">'1970-01-01 08:00:00'</span><span class="p">,</span> <span class="s1">'DateTime'</span><span class="p">)</span> <span class="k">COMMENT</span> <span class="s1">'Log time'</span><span class="p">,</span>
  <span class="nv">`action_id`</span> <span class="n">UInt16</span> <span class="k">DEFAULT</span> <span class="k">CAST</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">'UInt16'</span><span class="p">)</span> <span class="k">COMMENT</span> <span class="s1">'Log behavior type id'</span><span class="p">,</span>
  <span class="nv">`action_name`</span> <span class="n">String</span> <span class="k">DEFAULT</span> <span class="s1">''</span> <span class="k">COMMENT</span> <span class="s1">'Log behavior type name'</span><span class="p">,</span>
  <span class="nv">`region_name`</span> <span class="n">String</span> <span class="k">DEFAULT</span> <span class="s1">''</span> <span class="k">COMMENT</span> <span class="s1">'Region name'</span><span class="p">,</span>
  <span class="nv">`uid`</span> <span class="n">UInt64</span> <span class="k">DEFAULT</span> <span class="k">CAST</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">'UInt64'</span><span class="p">)</span> <span class="k">COMMENT</span> <span class="s1">'User id'</span><span class="p">,</span>
  <span class="nv">`level`</span> <span class="n">UInt32</span> <span class="k">DEFAULT</span> <span class="k">CAST</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">'UInt32'</span><span class="p">)</span> <span class="k">COMMENT</span> <span class="s1">'Current level'</span><span class="p">,</span>
  <span class="nv">`trans_no`</span> <span class="n">String</span> <span class="k">DEFAULT</span> <span class="s1">''</span> <span class="k">COMMENT</span> <span class="s1">'Transaction serial number'</span><span class="p">,</span>
  <span class="nv">`ext_head`</span> <span class="n">String</span> <span class="k">DEFAULT</span> <span class="s1">''</span> <span class="k">COMMENT</span> <span class="s1">'Extended log head'</span><span class="p">,</span>
  <span class="nv">`avatar_id`</span> <span class="n">UInt32</span> <span class="k">DEFAULT</span> <span class="k">CAST</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">'UInt32'</span><span class="p">)</span> <span class="k">COMMENT</span> <span class="s1">'Avatar id'</span><span class="p">,</span>
  <span class="nv">`scene_id`</span> <span class="n">UInt32</span> <span class="k">DEFAULT</span> <span class="k">CAST</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">'UInt32'</span><span class="p">)</span> <span class="k">COMMENT</span> <span class="s1">'Scene id'</span><span class="p">,</span>
  <span class="nv">`time_ts`</span> <span class="n">UInt64</span> <span class="k">DEFAULT</span> <span class="k">CAST</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">'UInt64'</span><span class="p">)</span> <span class="k">COMMENT</span> <span class="s1">'Second timestamp'</span><span class="p">,</span>
  <span class="k">index</span> <span class="n">avatar_id_minmax</span> <span class="p">(</span><span class="n">avatar_id</span><span class="p">)</span> <span class="k">type</span> <span class="n">minmax</span> <span class="n">granularity</span> <span class="mi">3</span>
<span class="p">)</span> <span class="n">ENGINE</span> <span class="o">=</span> <span class="n">MergeTree</span><span class="p">()</span>
<span class="k">PARTITION</span> <span class="k">BY</span> <span class="p">(</span><span class="n">toYYYYMMDD</span><span class="p">(</span><span class="nb">time</span><span class="p">),</span> <span class="n">toHour</span><span class="p">(</span><span class="nb">time</span><span class="p">),</span> <span class="n">region_name</span><span class="p">)</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="p">(</span><span class="n">action_id</span><span class="p">,</span> <span class="n">scene_id</span><span class="p">,</span> <span class="n">time_ts</span><span class="p">,</span> <span class="k">level</span><span class="p">,</span> <span class="n">uid</span><span class="p">)</span>
<span class="k">PRIMARY</span> <span class="k">KEY</span> <span class="p">(</span><span class="n">action_id</span><span class="p">,</span> <span class="n">scene_id</span><span class="p">,</span> <span class="n">time_ts</span><span class="p">,</span> <span class="k">level</span><span class="p">);</span>
</code></pre></div></div>

<p>The following figure shows the MergeTree storage structure logic of the table:</p>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/6d6d8bdf-be22-42c3-b935-64c09b87cbf2" alt="9b6058003d49ce37745344a93c5f3250ea276ba9_(1)" /></p>

<ul>
  <li>Partition</li>
</ul>

<p>A single partition contains multiple MergeTree Data Parts. In the storage structure of the MergeTree table, <strong>each data partition is independent of each other with no logical connections.</strong></p>

<ul>
  <li>Data Part</li>
</ul>

<p>A new MergeTree Data Part is generated for each batch insert operation.</p>

<p>Once these Data Parts are generated, they are immutable. The generation and destruction of Data Parts are mainly related <strong>to writing and asynchronous Merge (compaction).</strong></p>

<h3 id="data-part">Data Part</h3>

<p>Plz note that inside one data part, there are <strong>1) primary key index, 2) mark identifiers, 3) data files, 4) etc (minmax_idx, skipping index, …)</strong></p>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/90bf925f-4362-4b4d-8173-9a9b0cd89b7f" alt="4d88ee03dec493f26a091508d43f5c668238bc33-2_(1)" /></p>

<h4 id="primary-key-index-primaryidx">Primary Key Index (primary.idx)</h4>

<p>It is capable of quickly finding the primary key rows. It stores the primary key value of the start row in each Granule, while the data in MergeTree storage is strictly sorted according to the primary</p>

<h4 id="mark-identifiers-column_1mrk-column_2mrk-">Mark Identifiers (column_1.mrk, column_2.mrk, …)</h4>

<p>Mark identifier is related to two important concepts in MergeTree columnar storage, namely, Granule and Block.</p>

<ul>
  <li>Block</li>
</ul>

<p>Block is the compression unit for the column-store file. Each Block of a column-store file contains several Granules.</p>

<p>The specific number of Granules is controlled by the ‘min_compress_block_size’ parameter. It checks whether the current Block size has reached the set value when the data in a Granule is written in a Block. If so, the current Block is compressed and then written to the disk.</p>

<ul>
  <li>Granule</li>
</ul>

<p>Granule is a logical concept to divide data by rows.</p>

<p>In earlier versions, the amount of rows a Granule makes up is set by the ‘index_granularity’  parameter. It ensures that the sum size of all columns in a Granule does not exceed the specified value</p>

<ul>
  <li>Mark identifier</li>
</ul>

<p>Neither the data size nor the number of rows are fixed in the Block of MergeTree, and the Granule is not a fixed-length logical concept.</p>

<p>Therefore, additional information is needed to find a Granule quickly. <strong>Mark identifier files can provide the solution.</strong> It records the number of rows of each Granule and the offset of the Block where it locates in the column-store compressed file. It also records the offset of a Granule in the decompressed Block.</p>

<h4 id="data-files-column_1bin-column_2bin-">Data Files (column_1.bin, column_2.bin, …)</h4>

<p>column_1.bin, column_2.bin, and others are <strong>column-store files after the single column is compressed by block.</strong></p>

<p>To be more specific, a single column of data may correspond to multiple column-store files.</p>

<h2 id="mergetree-query">MergeTree Query</h2>

<p>It is roughly divided into two parts: index retrieval and data scanning.</p>

<h3 id="index-retrieval">Index Retrieval</h3>

<p>MergeTree storage extracts the KeyCondition of the partition key and the primary key in the query when receiving a select query.</p>

<ol>
  <li>prune irrelevant data partitions with the partition key KeyCondition first.</li>
  <li>select the rough Mark Ranges using the primary key index.</li>
  <li>filter the Mark Ranges generated by the primary key index with skipping index</li>
</ol>

<h3 id="data-scanning">Data Scanning</h3>

<p>MergeTree provides three different modes for data scanning: Final Mode, Sorted Mode, <strong>Normal Mode</strong></p>

<p>Data is scanned in parallel among multiple Data Parts, which can achieve very high data reading throughput for a single query.</p>

<p>The following describes several key performance optimizations in the Normal mode:</p>

<ul>
  <li><strong>Parallel Scanning</strong></li>
</ul>

<p>ClickHouse adds the Mark Range parallelism feature to the MergeTree Data Part parallelism feature. Users can set the parallelism in the data scanning process at will.</p>

<ul>
  <li>Data Cache</li>
  <li>SIMD Deserialization</li>
  <li>PreWhere Filtering</li>
</ul>

<h2 id="materialized-view">Materialized View</h2>

<p>https://clickhouse.com/docs/en/guides/developer/cascading-materialized-views</p>

<h2 id="optimization">Optimization</h2>

<h3 id="compression">Compression</h3>

<p>The compression rate is <strong>closely co-related to which pattern and how sparse and distributed given dataset is</strong>, but basically that we can expect to achieve significant amount of raw dataset would be compressed.</p>

<p>Examples.</p>

<ul>
  <li>uk_price_paid</li>
</ul>

<p>https://clickhouse.com/docs/en/getting-started/example-datasets/uk-price-paid/</p>

<p>7.54GB (raw) → 299.65MB (compressed) (25.76x)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0 rows in set. Elapsed: 848.875 sec. Processed 27.84 million rows, 7.54 GB (32.79 thousand rows/s., 8.88 MB/s.)
=&gt; 

SELECT formatReadableSize(total_bytes)
FROM system.tables
WHERE name = 'uk_price_paid'

Query id: edc4dddc-e1b1-49fe-9505-3576222162d2
┌─formatReadableSize(total_bytes)─┐
│ 299.65 MiB                      │
└─────────────────────────────────┘
</code></pre></div></div>

<ul>
  <li>nyc_taxi</li>
</ul>

<p>https://clickhouse.com/docs/en/getting-started/example-datasets/nyc-taxi</p>

<p>228.06MB (raw) → 121.23MB (compressed) (1.88x)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0 rows in set. Elapsed: 61.972 sec. Processed 3.00 million rows, 228.06 MB (48.41 thousand rows/s., 3.68 MB/s.)
=&gt; 

┌─formatReadableSize(total_bytes)─┐
│ 121.23 MiB                      │
└─────────────────────────────────┘
</code></pre></div></div>

<h2 id="use-cases">Use Cases</h2>

<p>https://clickhouse.com/docs/en/about-us/adopters</p>

<ul>
  <li>CloudFlare
    <ul>
      <li>All DNS AND HTTP logs (over 10M of rows/s)</li>
      <li>Moved from PostgreSQL to ClickHouse</li>
      <li>50+ servers, 120 TB per server</li>
      <li>https://blog.cloudflare.com/log-analytics-using-clickhouse/</li>
      <li>https://blog.cloudflare.com/http-analytics-for-6m-requests-per-second-using-clickhouse/</li>
    </ul>
  </li>
  <li>Uber
    <ul>
      <li>Centralized logging platform</li>
      <li>Moved from ELK to ClickHouse</li>
      <li>80% queries are aggregations</li>
      <li><a href="https://www.uber.com/en-KR/blog/logging/">Uber Engineering Blog - Fast and Reliable Schema-Agnostic Log Analytics Platform</a></li>
      <li>https://presentations.clickhouse.com/meetup40/uber.pdf</li>
    </ul>
  </li>
  <li>eBay
    <ul>
      <li>OLAP platform</li>
      <li>Moved from Druid to ClickHouse</li>
      <li>Reduced HW cost by over 90% - from 900 to 90 servers!</li>
      <li><a href="https://tech.ebayinc.com/engineering/ou-online-analytical-processing/">eBay Engineering Blog - Our Online Analytical Processing Journey with ClickHouse on Kubernetes</a></li>
    </ul>
  </li>
</ul>

<h2 id="reference">Reference</h2>

<ul>
  <li>Apache HBase, Google BigTable
    <ul>
      <li><a href="https://data-flair.training/blogs/hbase-architecture/">https://data-flair.training/blogs/hbase-architecture/</a></li>
      <li><a href="https://www.slideshare.net/quipo/nosql-databases-why-what-and-when/127-Google_BigTable_Architecture_fs_metadata">https://www.slideshare.net/quipo/nosql-databases-why-what-and-when/127-Google_BigTable_Architecture_fs_metadata</a></li>
    </ul>
  </li>
  <li>ClickHouse doc
    <ul>
      <li><a href="https://clickhouse.com/docs/en/home/">https://clickhouse.com/docs/en/home/</a></li>
      <li><a href="https://clickhouse.com/docs/en/faq/general/why-clickhouse-is-so-fast/">https://clickhouse.com/docs/en/faq/general/why-clickhouse-is-so-fast/</a></li>
      <li><a href="https://www.alibabacloud.com/blog/clickhouse-kernel-analysis-storage-structure-and-query-acceleration-of-mergetree_597727">https://www.alibabacloud.com/blog/clickhouse-kernel-analysis-storage-structure-and-query-acceleration-of-mergetree_597727</a></li>
      <li><a href="https://clickhouse.com/company/events/getting-started-with-clickhouse">https://clickhouse.com/company/events/getting-started-with-clickhouse</a></li>
    </ul>
  </li>
  <li>Conferences
    <ul>
      <li><a href="https://www.youtube.com/watch?v=6WICfakG84c">Secrets of ClickHouse Query Performance</a></li>
      <li><a href="https://www.youtube.com/watch?v=ZOZQCQEtrz8">The Secrets of ClickHouse Performance Optimizations at BDTC 2019</a></li>
      <li><a href="https://www.youtube.com/watch?v=fGG9dApIhDU">Introducing ClickHouse – The Fastest Data Warehouse You’ve Never Heard Of (Robert Hodges, Altinity)</a></li>
      <li><a href="https://www.youtube.com/watch?v=XpkFEj1rVXg">A Day in the Life of a ClickHouse Query — Intro to ClickHouse Internals ClickHouse Tutorial</a></li>
    </ul>
  </li>
</ul>

<h3 id="additional-ref">Additional Ref</h3>

<ul>
  <li><a href="https://clickhouse.com/docs/en/concepts/why-clickhouse-is-so-fast">https://clickhouse.com/docs/en/concepts/why-clickhouse-is-so-fast</a></li>
  <li><a href="https://altinity.com/blog/2020/1/1/clickhouse-cost-efficiency-in-action-analyzing-500-billion-rows-on-an-intel-nuc">https://altinity.com/blog/2020/1/1/clickhouse-cost-efficiency-in-action-analyzing-500-billion-rows-on-an-intel-nuc</a></li>
  <li><a href="http://www.cs.columbia.edu/~kar/pubsk/simd.pdf">http://www.cs.columbia.edu/~kar/pubsk/simd.pdf</a></li>
  <li>
    <p><a href="https://www.sciencedirect.com/topics/computer-science/single-instruction-multiple-data">https://www.sciencedirect.com/topics/computer-science/single-instruction-multiple-data</a></p>
  </li>
  <li>Blog
    <ul>
      <li><a href="https://clickhouse.com/blog/introduction-to-the-clickhouse-query-cache-and-design">https://clickhouse.com/blog/introduction-to-the-clickhouse-query-cache-and-design</a></li>
      <li><a href="https://clickhouse.com/blog/using-ttl-to-manage-data-lifecycles-in-clickhouse">https://clickhouse.com/blog/using-ttl-to-manage-data-lifecycles-in-clickhouse</a></li>
      <li><a href="https://clickhouse.com/blog/data-formats-clickhouse-csv-tsv-parquet-native">https://clickhouse.com/blog/data-formats-clickhouse-csv-tsv-parquet-native</a></li>
    </ul>
  </li>
</ul>]]></content><author><name>taehyeok-jang</name></author><category term="database" /><category term="clickhouse" /><category term="internal" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Apache Kafka - Skim Through Kafka Connect</title><link href="/jekyll-theme-yat/stream-processing/2023/03/20/apache-kafka-skim-through-kafka-connect.html" rel="alternate" type="text/html" title="Apache Kafka - Skim Through Kafka Connect" /><published>2023-03-20T00:00:00+00:00</published><updated>2023-03-20T00:00:00+00:00</updated><id>/jekyll-theme-yat/stream-processing/2023/03/20/apache-kafka-skim-through-kafka-connect</id><content type="html" xml:base="/jekyll-theme-yat/stream-processing/2023/03/20/apache-kafka-skim-through-kafka-connect.html"><![CDATA[<p>이번 글에서는 Kafka Connect를 소개하고 Kafka Connect를 도입하기 위해 필요한 고려사항을 살펴본 이후에, Kafka Connect를 사용한 간단한 파이프라인을 만들어보겠습니다.</p>

<h2 id="what-is-kafka-connect">What is Kafka Connect?</h2>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/1262f09a-5c1f-49b5-b5b3-1538c196112d" alt="ingest-data-upstream-systems" /></p>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/3bbde458-0427-4c7b-ba60-b743a3bc747b" alt="kafka_connect_architecture" /></p>

<p>Kafka Connect는 Apache Kafka의 컴포넌트들 중 하나로, database, 클라우드 서비스, 검색 인덱스, 파일 시스템, 키-값 저장소와 같은 다른 시스템과의 streaming을 수행하는 데 사용됩니다.</p>

<p>Kafka Connect를 사용하면 다양한 소스에서 Kafka로 데이터를 streaming하는 것(source)은 물론, Kafka에서 다양한 외부 시스템으로 데이터를 streaming하는 것(sink)이 쉬워집니다. 위의 다이어그램은 이러한 source와 sink가 가능한 대상 시스템들의 일부를 보여줍니다. Kafka Connect에는 외부 시스템과 Kafka Connect 간의 connection 및 streaming을 위한 수백 가지 다른 connector가 있습니다. d아래는 몇 가지 예시입니다.</p>

<ul>
  <li>RDBMS (Oracle, SQL Server, Db2, Postgres, MySQL)</li>
  <li>Cloud object stores (Amazon S3, Azure Blob Storage, Google Cloud Storage)</li>
  <li>NoSQL and document stores (Elasticsearch, MongoDB, Cassandra)</li>
  <li>Cloud data warehouses (Snowflake, Google BigQuery, Amazon Redshift)</li>
</ul>

<h2 id="related-kips-kafka-improvement-proposal">Related KIPs (Kafka Improvement Proposal)</h2>

<p>다음은 Kafka Connect의 기반 기술이 되는 Kafka의 여러 개선에 알아보겠습니다. KIP는 Kafka Improvement Proposal의 약자로 Apache Kafka 커뮤니티에서 사용하는 공식적인 프로세스입니다. Kafka의 여러 이슈들 중에서 major change에 해당하는 이슈에 대해 제안하고 논의하기 위해 사용됩니다.</p>

<h3 id="kip-98-exactly-once-delivery-and-transactional-messaging">KIP-98: Exactly Once Delivery and Transactional Messaging</h3>

<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging">https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging</a></p>

<p>이전 버전의 Kafka에서는 at-least-once semantics을 지원하여, producer가 retry를 하는 상황에서 broker에 메시지가 중복으로 저장되었습니다. 또한 부분 실패 시에 메세지가 일부 topic partition에만 저장되는 문제가 있었습니다.</p>

<p>하지만 producer id, transactional id 를 각각 도입하여, broker에서 중복 메시지를 제거하고 부분 실패 상황에서도 메시지를 transactional(all or nothing)한 방식으로 저장할 수 있게 되었습니다.</p>

<h3 id="kip-318-make-kafka-connect-source-idempotent">KIP-318: Make Kafka Connect Source idempotent</h3>

<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-318%3A+Make+Kafka+Connect+Source+idempotent">https://cwiki.apache.org/confluence/display/KAFKA/KIP-318%3A+Make+Kafka+Connect+Source+idempotent</a></p>

<p>Kafka의 exactly-once delivery, transactional messaging을 Kafka Connect에도 적용하였습니다.</p>

<h3 id="kip-618-exactly-once-support-for-source-connectors">KIP-618: Exactly-Once Support for Source Connectors</h3>

<p>https://cwiki.apache.org/confluence/display/KAFKA/KIP-618%3A+Exactly-Once+Support+for+Source+Connectors</p>

<p>위 개선들을 통해 Kafka Connect에서 sink에 대한 exactly-once delivery는 지원이 가능했지만 source에서는 여전히 불가능했었습니다.</p>

<p>이번 개선에서는 atomic offset writes, per-connector offset topics을 통해 source에서도 exactly-once delivery가 가능하도록 하였습니다.</p>

<h2 id="kafka-connect-vs-own-kafka-producerconsumer">Kafka Connect vs Own Kafka Producer/Consumer?</h2>

<p>Kafka Connect을 도입하기에 앞서 다음과 같은 고민이 들 수도 있습니다. 데이터 처리 스트리밍을 구현하는 것은 Kafka Producer/Consumer를 조합하는 방식으로도 가능한데 별도의 플랫폼을 구축하는 것이 필요한지? 개발 팀에게 익숙한 Kafka 스택을 넘어서 굳이 새로운 기술을 채택해야하는지?</p>

<p>하지만 Kafka를 데이터 저장소에 연결하는 앱을 작성하는 것은 간단해 보이지만, 이 과정에서 고려해야 할 점들이 다양하게 있습니다. 아래 문제들은 application에서 producer/consumer을 통해 직접 구현했을 때 해결해야 할 문제들이며 이를 개발하고 검증하려면 오랜 노력이 필요합니다.</p>

<ul>
  <li>부분 실패 및 재시작 처리</li>
  <li>logging</li>
  <li>data load에 따른 scale up/down</li>
  <li>분산 모드에서 실행</li>
  <li>Serialization 및 data format</li>
</ul>

<p>Kafka Connect는 이미 위의 모든 문제를 해결하면서 성장한 framework입니다. 또한 configuration 관리, offset storage, parallelization, error handling, 다양한 데이터 유형 지원 및 표준 관리 REST API와 기능을 제공합니다. 따라서 단순히 어플리케이션 로직 처리가 아니라 <strong>스트리밍 플랫폼을 구축하기를 원한다면 Kafka Connect</strong>을 사용하는 것을 추천합니다.</p>

<h2 id="cdc-by-query-based-vs-log-based">CDC by Query-based vs Log-based</h2>

<p>대표적인 관계형 데이터베이스인 MySQL에 Kafka Connect를 연결하기 위한 connector로 두가지를 찾을 수 있습니다.</p>

<ul>
  <li>JDBC Connector (query-based)</li>
  <li>Debezium Connector (log-based)</li>
</ul>

<p>위 connector들은 데이터 저장소에서 데이터 변화를 감지(CDC, changed data capture)하기 위한 각각의 방식을 채택하고 있으며 아래 장단점들을 가지고 있습니다. Query-based는 비교적 쉬운 설정 및 낮은 비용이 장점인 반면, Log-based는 DB에 별도의 부하가 없는 점, 실시간 데이터 변화 감지 등이 장점입니다.</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>Query-based</th>
      <th>Log-based</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Method</td>
      <td>Polling DB with a query like,<br /> ‘SELECT * <br />FROM table<br />WHERE updatedat BETWEEN ‘now() - 1 hour’ AND ‘now()’</td>
      <td>Transaction log (Ex. MySQL binlog) based streaming.<br />Retrieve dump from master and parse binary log objects.</td>
    </tr>
    <tr>
      <td>Pros</td>
      <td>- Usually easier to setup</td>
      <td>- All data changes are captured ﻿﻿<br />- Can capture DELETE﻿<br />- Real-time capture while avoiding increased CPU load</td>
    </tr>
    <tr>
      <td>Cons</td>
      <td>- ﻿﻿Impact of polling DB ﻿﻿<br />- Can’t track events between polling interval<br />- Can’t track DELETE</td>
      <td>- More setup steps <br />- ﻿﻿Higher system previleges required</td>
    </tr>
    <tr>
      <td>Projects</td>
      <td>kafka-connect-jdbc,…</td>
      <td>https://debezium.io/<br />https://github.com/alibaba/canal</td>
    </tr>
    <tr>
      <td>,</td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h2 id="kafka-connect-실습">Kafka Connect 실습</h2>

<p>Kafka Connect을 통해 file stream을 읽는 스트리밍 파이프라인을 구성해보겠습니다. 실습은 Udemy의 Kafka Connect 강의를 참고하였습니다.</p>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/6f95fea4-5057-4e09-8e6d-64eb65f7220e" alt="udemy_apache_kafka_connect" /></p>

<p>Kafka Connect는 standalone, distributed mode로 실행이 가능하며 실습에서는 distributed mode로 실행하겠습니다.</p>

<p>Kafka Connect 설정 파일입니다.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># config/connect-distributed.properties</span>

bootstrap.servers<span class="o">=</span>ec2-kafka:9092
group.id<span class="o">=</span>connect-cluster

key.converter<span class="o">=</span>org.apache.kafka.connect.json.JsonConverter
value.converter<span class="o">=</span>org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable<span class="o">=</span><span class="nb">true
</span>value.converter.schemas.enable<span class="o">=</span><span class="nb">true

</span>offset.storage.topic<span class="o">=</span>connect-offsets
offset.storage.replication.factor<span class="o">=</span>1

config.storage.topic<span class="o">=</span>connect-configs
config.storage.replication.factor<span class="o">=</span>

status.storage.topic<span class="o">=</span>connect-status
status.storage.replication.factor<span class="o">=</span>1

offset.flush.interval.ms<span class="o">=</span>10000
</code></pre></div></div>

<p>Kafka Connect를 distributed mode로 실행합니다.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> bin/connect-distributed.sh config/connect-distributed.properties

<span class="o">&gt;</span> curl <span class="nt">-s</span> localhost:8083
<span class="o">{</span><span class="s2">"version"</span>:<span class="s2">"2.5.0"</span>,<span class="s2">"commit"</span>:<span class="s2">"&lt;commit&gt;"</span>,<span class="s2">"kafka_cluster_id"</span>:<span class="s2">"&lt;cluster_id&gt;"</span><span class="o">}</span>
<span class="o">&gt;</span> curl <span class="nt">-s</span> localhost:8083/connector-plugins
<span class="o">[{</span><span class="s2">"class"</span>:<span class="s2">"org.apache.kafka.connect.file.FileStreamSinkConnector"</span>,<span class="s2">"type"</span>:<span class="s2">"sink"</span>,<span class="s2">"version"</span>:<span class="s2">"2.5.0"</span><span class="o">}</span>,
<span class="o">{</span><span class="s2">"class"</span>:<span class="s2">"org.apache.kafka.connect.file.FileStreamSourceConnector"</span>,<span class="s2">"type"</span>:<span class="s2">"source"</span>,<span class="s2">"version"</span>:<span class="s2">"2.5.0"</span><span class="o">}</span>,
<span class="o">{</span><span class="s2">"class"</span>:<span class="s2">"org.apache.kafka.connect.mirror.MirrorCheckpointConnector"</span>,<span class="s2">"type"</span>:<span class="s2">"source"</span>,<span class="s2">"version"</span>:<span class="s2">"1"</span><span class="o">}</span>,
<span class="o">{</span><span class="s2">"class"</span>:<span class="s2">"org.apache.kafka.connect.mirror.MirrorHeartbeatConnector"</span>,<span class="s2">"type"</span>:<span class="s2">"source"</span>,<span class="s2">"version"</span>:<span class="s2">"1"</span><span class="o">}</span>,
<span class="o">{</span><span class="s2">"class"</span>:<span class="s2">"org.apache.kafka.connect.mirror.MirrorSourceConnector"</span>,<span class="s2">"type"</span>:<span class="s2">"source"</span>,<span class="s2">"version"</span>:<span class="s2">"1"</span><span class="o">}]</span>
</code></pre></div></div>

<p>Kafka Connect가 실행되었으면 FileStreamSourceConnector 등록합니다. source에서 데이터 스트림을 읽어서 ‘connect-test’ topic에 저장합니다.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">--request</span> POST <span class="s1">'localhost:8083/connectors'</span> <span class="se">\</span>
<span class="nt">--header</span> <span class="s1">'Content-Type: application/json'</span> <span class="se">\</span>
<span class="nt">--data-raw</span> <span class="s1">'{
    "name": "file-source-connector",
    "config": {
        "connector.class": "org.apache.kafka.connect.file.FileStreamSourceConnector",
        "tasks.max": "1",
        "topic": "connect-test",
        "file": "~/test.txt"
    }
}'</span>
</code></pre></div></div>

<p>파일에 텍스트를 입력합니다.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="nb">echo </span>hello <span class="o">&gt;&gt;</span> test.txt
<span class="o">&gt;</span> <span class="nb">echo </span>world <span class="o">&gt;&gt;</span> test.txt
<span class="o">&gt;</span> <span class="nb">echo</span> <span class="o">!!!</span> <span class="o">&gt;&gt;</span> test.txt
<span class="o">&gt;</span> <span class="nb">echo</span> ??? <span class="o">&gt;&gt;</span> test.txt
</code></pre></div></div>

<p>데이터가 source로부터 잘 입력되었는지 확인합니다.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> ./kafka-console-consumer.sh <span class="nt">--bootstrap-server</span> ec2-kafka:9092 <span class="nt">--topic</span> connect-test <span class="nt">--from-beginning</span>                   
<span class="o">{</span><span class="s2">"schema"</span>:<span class="o">{</span><span class="s2">"type"</span>:<span class="s2">"string"</span>,<span class="s2">"optional"</span>:false<span class="o">}</span>,<span class="s2">"payload"</span>:<span class="s2">"hello"</span><span class="o">}</span>
<span class="o">{</span><span class="s2">"schema"</span>:<span class="o">{</span><span class="s2">"type"</span>:<span class="s2">"string"</span>,<span class="s2">"optional"</span>:false<span class="o">}</span>,<span class="s2">"payload"</span>:<span class="s2">"world"</span><span class="o">}</span>
<span class="o">{</span><span class="s2">"schema"</span>:<span class="o">{</span><span class="s2">"type"</span>:<span class="s2">"string"</span>,<span class="s2">"optional"</span>:false<span class="o">}</span>,<span class="s2">"payload"</span>:<span class="s2">"!!!"</span><span class="o">}</span>
<span class="o">{</span><span class="s2">"schema"</span>:<span class="o">{</span><span class="s2">"type"</span>:<span class="s2">"string"</span>,<span class="s2">"optional"</span>:false<span class="o">}</span>,<span class="s2">"payload"</span>:<span class="s2">"???"</span><span class="o">}</span>
</code></pre></div></div>

<h2 id="references">References</h2>

<ul>
  <li>What is Kafka Connect?
    <ul>
      <li><a href="https://developer.confluent.io/courses/kafka-connect/intro/">https://developer.confluent.io/courses/kafka-connect/intro/</a></li>
    </ul>
  </li>
  <li>Related KIPs (Kafka Improvement Proposal)
    <ul>
      <li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals">Kafka Improvement Proposal</a></li>
    </ul>
  </li>
  <li>Kafka Connect vs Own Kafka Producer/Consumer?
    <ul>
      <li><a href="https://stackoverflow.com/questions/59495694/kafka-design-questions-kafka-connect-vs-own-consumer-producer">https://stackoverflow.com/questions/59495694/kafka-design-questions-kafka-connect-vs-own-consumer-producer</a></li>
      <li><a href="https://medium.com/@stephane.maarek/the-kafka-api-battle-producer-vs-consumer-vs-kafka-connect-vs-kafka-streams-vs-ksql-ef584274c1e">https://medium.com/@stephane.maarek/the-kafka-api-battle-producer-vs-consumer-vs-kafka-connect-vs-kafka-streams-vs-ksql-ef584274c1e</a></li>
      <li><a href="https://community.cloudera.com/t5/Support-Questions/When-to-use-Kafka-Connect-vs-Producer-and-Consumer/m-p/160485#M122870">https://community.cloudera.com/t5/Support-Questions/When-to-use-Kafka-Connect-vs-Producer-and-Consumer/m-p/160485#M122870</a></li>
    </ul>
  </li>
  <li>CDC by Query-based vs Log-based
    <ul>
      <li><a href="https://stackoverflow.com/questions/65612131/usability-of-binary-log-in-data-streaming-in-mysql-what-are-the-drawbacks-and-a">https://stackoverflow.com/questions/65612131/usability-of-binary-log-in-data-streaming-in-mysql-what-are-the-drawbacks-and-a</a></li>
    </ul>
  </li>
  <li>Kafka Connect 실습
    <ul>
      <li><a href="https://www.udemy.com/course/kafka-connect/">https://www.udemy.com/course/kafka-connect/</a></li>
    </ul>
  </li>
</ul>]]></content><author><name>taehyeok-jang</name></author><category term="stream-processing" /><category term="kafka" /><summary type="html"><![CDATA[이번 글에서는 Kafka Connect를 소개하고 Kafka Connect를 도입하기 위해 필요한 고려사항을 살펴본 이후에, Kafka Connect를 사용한 간단한 파이프라인을 만들어보겠습니다.]]></summary></entry><entry><title type="html">Apache Kafka - Troubleshoot</title><link href="/jekyll-theme-yat/stream-processing/2022/06/01/apache-kafka-troubleshoot.html" rel="alternate" type="text/html" title="Apache Kafka - Troubleshoot" /><published>2022-06-01T00:00:00+00:00</published><updated>2022-06-01T00:00:00+00:00</updated><id>/jekyll-theme-yat/stream-processing/2022/06/01/apache-kafka-troubleshoot</id><content type="html" xml:base="/jekyll-theme-yat/stream-processing/2022/06/01/apache-kafka-troubleshoot.html"><![CDATA[<p>이 글은 Apache Kafka를 사용하면서 발생할 수 있는 문제들과 그 해결책에 대해서 정리한 글입니다.</p>

<h2 id="producer에서-발생하는-문제">Producer에서 발생하는 문제</h2>

<h3 id="notleaderforpartitionexception-this-server-is-not-the-leader-for-that-topic-partition-going-to-request-metadata-update-now">NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>WARN [Producer clientId=test-producer] Received invalid metadata error in produce request on partition &lt;topic&gt; due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender)
</code></pre></div></div>

<h4 id="cause">Cause</h4>

<p>Kafka producer에서 produce 및 fetch 요청은 partition의 leader replica로 보내집니다. NotLeaderForPartitionException 예외는 해당 요청이 현재 partition의 leader replica가 아닌 곳으로 보내질 때 발생합니다.</p>

<p>클라이언트는 각 partition의 leader에 대한 metadata 정보를 캐시로 유지합니다. 캐시 관리의 전체 과정은 아래에 나와 있습니다. producer config에서 <code class="language-plaintext highlighter-rouge">metadata.max.age.ms</code>(default 5분)을 설정하여 아래 과정이 정기적으로 발생하도록 해야합니다.</p>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/37c27ac2-897b-4cc3-b392-5b54c843a272" alt="apache_kafka_metadata_req" /></p>

<h4 id="resolution">Resolution</h4>

<p>이 문제는 실제로 개발자가 직접 이 문제를 해결할 필요는 없습니다.</p>

<p>이는 Kafka cluster 운영 중 old node에서 new node로의 failover의 일부로 예상되는 동작이기 때문입니다. 클러스터 내 node가 교체되면 partition leader가 변경될 것으로 예상됩니다. 모든 partition leader는 한 번 이상 변경되지만 node 수와 한 번에 교체되는 node 수에 따라 더 많이 변경될 수 있습니다.</p>

<p>또한 producer에서는 주기적 poll (metadata.max.age.ms 마다) 또는 NotLeaderForPartitionException가 발생한 즉시 cluster metadata 캐시를 업데이트하여 문제 없이 메시지를 produce 합니다. 하지만 종종 broker 측의 문제로 인해 (overloaded throughput, node replacement requests in parallel) 동시에 여러 node 교체가 발생할 수 있습니다. 이때 마찬가지로 동일한 문제가 발생할 수 있지만 실제로는 영향이 없습니다.</p>

<h2 id="consumer에서-발생하는-문제">Consumer에서 발생하는 문제</h2>

<h3 id="commitfailedexception-commit-cannot-be-completed-since-the-group-has-already-rebalanced-and-assigned-the-partitions-to-another-member">CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:775)
</code></pre></div></div>

<h4 id="cause-1">Cause</h4>

<p>Kafka consumer에서 메시지를 poll하여 처리하는 도중에, consumer group에서 rebalancing 이 발생하여 해당 consumer가 consumer group에서 제외되어 commit에 실패했습니다.</p>

<p>consumer application 운영 중 이 에러가 처음 발생했을 때 일시적인 문제라고 생각하고 대수롭지 않게 여겼습니다. 분산 시스템에서는 네트워크 지연 등으로 인한 부분 실패가 빈번하게 발생할 수 있고, 이를 대비하여 consumer 로직을 at-least-once 방식으로 구현하여 commit에 실패한 메시지들이 금방 재시도 처리될 것이기 때문입니다. 하지만 지속적으로 이 문제가 발생하여 원인에 대해 제대로 살펴보고 해결하기로 했습니다.</p>

<h4 id="rebalance">Rebalance?</h4>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/b3f3865c-2b27-4595-a9e4-90cda5312a1b" alt="apache_kafka_rebalancing" /></p>

<p>(figure. rebalancing process)</p>

<p>rebalance는 consumer group들을 재조정하는 작업을 의미합니다. 새로운 consumer가 consumer group에 추가되거나, consumer consumer에서 특정한 consumer가 제외되었을 때 각 consumer들이 소비하고 있는 partition들에 대한 재조정이 필요한데 이를 rebalancing 이라 합니다.</p>

<p>consumer가 소속 consumer group에서 제외되는 rebalancing이 발생하는 원인은 다음의 두가지입니다.</p>

<ul>
  <li>consumer의 메시지 처리 지연으로 인해 poll이 호출되는 간격이 <code class="language-plaintext highlighter-rouge">max.poll.interval.ms</code>를 초과하는 경우</li>
  <li>consumer 인스턴스 자체의 문제로 GroupCoordinator가 <code class="language-plaintext highlighter-rouge">session.timeout.ms</code> 이내에 heartbeat를 수신 받지 못한 경우 (heartbeat signal은 consumer에서 3초(default)에 한번씩 주기적으로 broker에 보내집니다)</li>
</ul>

<p>위의 상황에서 Kafka의 GroupCoordinator는 해당 consumer에 문제가 있는 것으로 판단하여 consumer group에서 제외시킵니다.</p>

<h4 id="resolution-1">Resolution</h4>

<p>consumer의 메시지 처리 지연의 경우 문제 해결을 위해 두가지 접근이 가능합니다. 먼저 한번의 poll로 가져오는 메시지의 수를 줄이는 것입니다. <code class="language-plaintext highlighter-rouge">max.poll.records</code> (default 500)으로 개수를 조정할 수 있습니다. 두번째는 poll을 호출되는 간격을 늘리는 것입니다. <code class="language-plaintext highlighter-rouge">max.poll.interval.ms</code> (default 5분)를 더 높은 값으로 설정합니다.</p>

<p>GroupCoordinator가 heartbeat를 제대로 수신 받지 못한 경우에는 consumer가 heartbeat를 더 자주 보내도록 하거나 GroupCoordinator에게 조금 더 신호를 오래 기다리도록 할 수 있습니다. <code class="language-plaintext highlighter-rouge">heartbeat.interval.ms</code> (default 2초)를 더 낮은 값으로, <code class="language-plaintext highlighter-rouge">session.timeout.ms</code> (default 10초)를 더 높은 값으로 설정합니다.</p>

<h4 id="trade-off">Trade-off</h4>

<p>위의 제안에 따라 설정들을 조정함으로써 consumer가 consumer group으로부터 제외되는 상황을 방지할 수 있지만 모든 상황에서 맞는 해결 방법은 아닙니다.</p>

<p>consumer의 메시지 처리 지연의 경우 <code class="language-plaintext highlighter-rouge">max.poll.records</code> 를 너무 낮게 설정하면 (ex. max.poll.records=1) consumer와 broker 간 요청이 증가하여 broker에 네트워크 부하가 발생합니다. 또 메시지 처리 지연 문제 자체를 해결할 필요가 있습니다. consumer application에서 long query 등의 작업으로 인해 한 개의 메시지를 처리를 처리하는데 오랜 시간이 걸린다면 이를 해결하는 것이 좋겠습니다. 만약 message throughput이 굉장히 높은 상황임에도 불구하고, commit 실패가 반복해서 일어난다고 <code class="language-plaintext highlighter-rouge">max.poll.records</code>는 작게, <code class="language-plaintext highlighter-rouge">max.poll.interval.ms</code>를 높게 설정하면 해당 topic의 lag으로 이어질 수도 있습니다.</p>

<p><img src="https://github.com/taehyeok-jang/taehyeok-jang.github.io/assets/31732943/dc28c892-1f62-4e56-8d67-5e60601c08dc" alt="apache_kafka_consumer_lag_metric" /></p>

<p>heartbeat와 관련된 문제에서도 주의가 필요합니다. consumer 어플리케이션이 실제로 shut-down 된 상황에서도 GroupCoordinator는 <code class="language-plaintext highlighter-rouge">session.timeout.ms</code> 만큼 기다리게되므로 무작정 값을 높게 설정했다가는 rebalancing을 하기까지 불필요한 시간이 소요될 수 있습니다. 따라서 서버의 성능이나 어플리케이션의 상황에 맞춰서 적정하게 값을 조정하는 것이 좋겠습니다.</p>

<h2 id="references">References</h2>

<ul>
  <li>Apache Kafka offical document
    <ul>
      <li><a href="https://kafka.apache.org/documentation">https://kafka.apache.org/documentation</a></li>
    </ul>
  </li>
  <li>Kafka Internal
    <ul>
      <li><a href="https://d2.naver.com/helloworld/0974525">KafkaConsumer Client Internals - NAVER D2</a></li>
    </ul>
  </li>
  <li>Problems in Producer
    <ul>
      <li><a href="https://stackoverflow.com/questions/61798565/kafka-producer-fails-to-send-messages-with-not-leader-for-partition-exception">Stack Overflow - Kafka producer fails to send messages with NOT_LEADER_FOR_PARTITION exception</a></li>
    </ul>
  </li>
  <li>Problems in Consumer
    <ul>
      <li><a href="https://www.confluent.io/blog/kafka-lag-monitoring-and-metrics-at-appsflyer/">https://www.confluent.io/blog/kafka-lag-monitoring-and-metrics-at-appsflyer/</a></li>
      <li><a href="https://github.com/ClickHouse/ClickHouse/issues/44884">https://github.com/ClickHouse/ClickHouse/issues/44884</a></li>
    </ul>
  </li>
</ul>]]></content><author><name>taehyeok-jang</name></author><category term="stream-processing" /><category term="kafka" /><category term="troubleshoot" /><summary type="html"><![CDATA[이 글은 Apache Kafka를 사용하면서 발생할 수 있는 문제들과 그 해결책에 대해서 정리한 글입니다.]]></summary></entry><entry><title type="html">Java Date Time API</title><link href="/jekyll-theme-yat/java/2022/02/19/java-date-time-api.html" rel="alternate" type="text/html" title="Java Date Time API" /><published>2022-02-19T00:00:00+00:00</published><updated>2022-02-19T00:00:00+00:00</updated><id>/jekyll-theme-yat/java/2022/02/19/java-date-time-api</id><content type="html" xml:base="/jekyll-theme-yat/java/2022/02/19/java-date-time-api.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Java에서 날짜 및 시간과 관련된 작업을 하다보면 Date, Calendar, Instant, ZonedDateTime 등 비슷한 목적의 여러 클래스들이 있어서 헷갈렸던 적이 있을 것입니다. 이는 Java 1.0에서 Date 클래스가 처음 도입된 이후로 여러가지 문제점들이 발견되어 이를 개선하기 위한 클래스들이 새로 생겨났기 때문인데요. Joda Time과 같은 오픈소스 라이브러리가 개발되기도 했지만 Java 8의 java.time 패키지가 도입되면서 Java 기본 패키지에서도 충분한 기능을 제공할 수 있게 되었습니다.</p>

<p>이번 글에서는 Java의 역사에서 관련 클래스들이 어떻게 발전했는지를 살펴보고, 그리고 나아가 어플리케이션에서 날짜 및 시간의 일관성을 지키기 위한 규칙에 대해서도 간단히 알아보겠습니다.</p>

<h2 id="a-brief-history">A Brief History</h2>

<h3 id="javautildate-java-10">java.util.Date (Java 1.0)</h3>

<p><a href="https://docs.oracle.com/javase/8/docs/api/java/util/Date.html">https://docs.oracle.com/javase/8/docs/api/java/util/Date.html</a></p>

<p>가장 먼저 출시된 클래스는 Java 1.0의 java.util.Date 클래스입니다. 공식 문서의 정의대로 이 클래스는 단순 날짜가 아니라 특정한 시간을 millisecond 단위로 나타냅니다. Date 클래스는 기본적인 사용 방법과 유효성 측면에서 여러 문제가 있었습니다.</p>

<p>첫번째로, 연도는 1900년부터 시작되었고 월은 0부터 시작하여 직관적인 사용이 어려웠습니다. 예를 들어 2020년 06월 10일을 나타내기 위해서는 아래와 같이 Date 인스턴스를 생성해야 합니다.</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">Date</span> <span class="n">date</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Date</span><span class="o">(</span><span class="mi">120</span><span class="o">,</span> <span class="mi">5</span><span class="o">,</span> <span class="mi">10</span><span class="o">);</span>
</code></pre></div></div>

<p>두번째로, 입력된 날짜에 대한 유효성 검증이 제대로 지원되지 않았습니다. 아래의 예시처럼 생성자에 2020년 05월 31일을 입력하면 아무런 예외나 경고 메시지 없이 2020년 06월 01일로 Date 인스턴스가 생성됩니다.</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">Date</span> <span class="n">date1</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Date</span><span class="o">(</span><span class="mi">120</span><span class="o">,</span> <span class="mi">5</span><span class="o">,</span> <span class="mi">31</span><span class="o">);</span>
<span class="nc">Date</span> <span class="n">date2</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Date</span><span class="o">(</span><span class="mi">120</span><span class="o">,</span> <span class="mi">6</span><span class="o">,</span> <span class="mi">1</span><span class="o">);</span>

<span class="n">assertEquals</span><span class="o">(</span><span class="n">date1</span><span class="o">,</span> <span class="n">date2</span><span class="o">);</span>
</code></pre></div></div>

<h3 id="javautilcalendar-java-11">java.util.Calendar (Java 1.1)</h3>

<p>Java 1.1에서 java.util.Calendar 클래스가 도입되면서 Date 클래스와 관련된 몇가지 문제점들이 개선되었지만 여전히 여러 문제점들이 있었습니다.</p>

<p>월이 여전히 0부터 시작합니다. Caldenar 클래스에서 <code class="language-plaintext highlighter-rouge">Calendar.JULY</code> 와 같은 상수들을 지원했지만 정수를 그대로 사용할 수 있는 이상 잘못 사용될 가능성은 여전히 존재합니다.</p>

<p>두번째로, Date와 마찬가지로 Calendar 클래스 또한 mutable이어서 set method로 값을 변경할 수 있습니다. 이 클래스들의 인스턴스가 여러 객체에서 공유되거나 여러 스레드에서 동시접근한다면 예상하지 못한 문제가 발생할 수 있습니다.</p>

<p>Time zone을 관리하기가 어렵습니다. TimeZone 인스턴스 생성 시 <code class="language-plaintext highlighter-rouge">Asia/Seoul</code> 이 아니라 실수로 <code class="language-plaintext highlighter-rouge">Seoul/Asia</code> 를 입력하면 아무런 예외가 발생하지 않으면서 Time zone은 <code class="language-plaintext highlighter-rouge">GMT</code> 로 생성됩니다. 이는 잠재적인 오류의 가능성을 가지고 있습니다.</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">TimeZone</span> <span class="n">zone</span> <span class="o">=</span> <span class="nc">TimeZone</span><span class="o">.</span><span class="na">getTimeZone</span><span class="o">(</span><span class="s">"Seoul/Asia"</span><span class="o">);</span>
<span class="nc">Calendar</span> <span class="n">calendar</span> <span class="o">=</span> <span class="nc">Calendar</span><span class="o">.</span><span class="na">getInstance</span><span class="o">(</span><span class="n">zone</span><span class="o">);</span> 

<span class="n">assertEquals</span><span class="o">(</span><span class="n">calendar</span><span class="o">.</span><span class="na">getTimeZone</span><span class="o">(),</span> <span class="s">"GMT"</span><span class="o">);</span>
</code></pre></div></div>

<h3 id="javatime-java-8-jsr-310">java.time (Java 8, JSR 310)</h3>

<p>Java 8에서 JSR 310 표준을 통합하여 java.time 패키지를 도입했습니다. 이 패키지는 기존 클래스들의 문제들을 해결하고 날짜와 시간을 관리하기 위한 기능을 제공합니다.</p>

<ol>
  <li>충분히 효율적인 API를 제공합니다.</li>
  <li>Date, Time, Instant, TimeZone을 위한 표준을 지원합니다.</li>
  <li>객체 간 공유 및 스레드 안정성을 위해 immutable로 구현하였습니다.</li>
</ol>

<p><a href="https://docs.oracle.com/javase/8/docs/api/java/time/package-summary.html">https://docs.oracle.com/javase/8/docs/api/java/time/package-summary.html</a></p>

<p>아래는 java.time 패키지 내 일부 클래스들입니다. 타임라인 상에서 특정 순간을 나타내기 위한 Instant 클래스, time zone 없이 날짜, 시간을 표기하기 위한 LocalDate, LocalTime, LocalDateTime 클래스가 있습니다. 그리고 time zone을 offset이나 id로 (ZoneOffset, ZoneId) 나타낼 수 있으며 OffsetDateTime, ZonedDateTime 클래스를 통해서 특정 지역에서 날짜와 시간과 관련된 완전한 정보를 표현할 수 있습니다. 모든 클래스는 날짜와 시간과 관련된 국제 표준인 <a href="https://en.wikipedia.org/wiki/ISO_8601">ISO-8601</a>을 따르고 있습니다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Class</th>
      <th style="text-align: left">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">[Instant](https://docs.oracle.com/javase/8/docs/api/java/time/Instant.html)</td>
      <td style="text-align: left">An instantaneous point on the time-line.</td>
    </tr>
    <tr>
      <td style="text-align: left">[LocalDate](https://docs.oracle.com/javase/8/docs/api/java/time/LocalDate.html)</td>
      <td style="text-align: left">A date without a time-zone in the ISO-8601 calendar system, such as <code class="language-plaintext highlighter-rouge">2007-12-03</code>.</td>
    </tr>
    <tr>
      <td style="text-align: left">[LocalDateTime](https://docs.oracle.com/javase/8/docs/api/java/time/LocalDateTime.html)</td>
      <td style="text-align: left">A date-time without a time-zone in the ISO-8601 calendar system, such as <code class="language-plaintext highlighter-rouge">2007-12-03T10:15:30</code>.</td>
    </tr>
    <tr>
      <td style="text-align: left">[LocalTime](https://docs.oracle.com/javase/8/docs/api/java/time/LocalTime.html)</td>
      <td style="text-align: left">A time without a time-zone in the ISO-8601 calendar system, such as <code class="language-plaintext highlighter-rouge">10:15:30</code>.</td>
    </tr>
    <tr>
      <td style="text-align: left">[OffsetDateTime](https://docs.oracle.com/javase/8/docs/api/java/time/OffsetDateTime.html)</td>
      <td style="text-align: left">A date-time with an offset from UTC/Greenwich in the ISO-8601 calendar system, such as <code class="language-plaintext highlighter-rouge">2007-12-03T10:15:30+01:00</code>.</td>
    </tr>
    <tr>
      <td style="text-align: left">[OffsetTime](https://docs.oracle.com/javase/8/docs/api/java/time/OffsetTime.html)</td>
      <td style="text-align: left">A time with an offset from UTC/Greenwich in the ISO-8601 calendar system, such as <code class="language-plaintext highlighter-rouge">10:15:30+01:00</code>.</td>
    </tr>
    <tr>
      <td style="text-align: left">[ZonedDateTime](https://docs.oracle.com/javase/8/docs/api/java/time/ZonedDateTime.html)</td>
      <td style="text-align: left">A date-time with a time-zone in the ISO-8601 calendar system, such as <code class="language-plaintext highlighter-rouge">2007-12-03T10:15:30+01:00 Europe/Paris</code>.</td>
    </tr>
    <tr>
      <td style="text-align: left">[ZoneId](https://docs.oracle.com/javase/8/docs/api/java/time/ZoneId.html)</td>
      <td style="text-align: left">A time-zone ID, such as <code class="language-plaintext highlighter-rouge">Europe/Paris</code>.</td>
    </tr>
    <tr>
      <td style="text-align: left">[ZoneOffset](https://docs.oracle.com/javase/8/docs/api/java/time/ZoneOffset.html)</td>
      <td style="text-align: left">A time-zone offset from Greenwich/UTC, such as <code class="language-plaintext highlighter-rouge">+02:00</code>.</td>
    </tr>
  </tbody>
</table>

<h2 id="date-time-apis">Date, Time APIs</h2>

<p>Java에서 날짜, 시간과 관련하여 여러 유용한 API들을 소개합니다. 레거시를 포함하는 시스템에서는 관련된 여러 클래스들이 섞여서 사용되고 있을텐데요. 하위 호환을 반드시 지원해야 하는 경우가 아니라면 java.time 패키지에 있는 클래스들로 변환하여 날짜와 시간을 명확하게 표현하는 것을 권장합니다.</p>

<h3 id="current-date-time">Current Date, Time</h3>

<ul>
  <li>Date</li>
</ul>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">new</span> <span class="nc">Date</span><span class="o">();</span>
</code></pre></div></div>

<ul>
  <li>LocalDate, LocalDateTime</li>
</ul>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">LocalDate</span><span class="o">.</span><span class="na">now</span><span class="o">();</span>
<span class="nc">LocalTime</span><span class="o">.</span><span class="na">now</span><span class="o">();</span>
<span class="nc">LocalDateTime</span><span class="o">.</span><span class="na">now</span><span class="o">();</span>
</code></pre></div></div>

<h3 id="convert-between-old--new">Convert between Old &amp; New</h3>

<ul>
  <li>Date -&gt; ZonedDateTime</li>
</ul>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// zoneId = ZoneId.systemDefault();</span>
<span class="kd">public</span> <span class="kd">static</span> <span class="nc">ZonedDateTime</span> <span class="nf">convert</span><span class="o">(</span><span class="nd">@Nonnull</span> <span class="nc">Date</span> <span class="n">date</span><span class="o">,</span> <span class="nc">ZoneId</span> <span class="n">zoneId</span><span class="o">)</span> <span class="o">{</span>
  <span class="k">return</span> <span class="n">date</span><span class="o">.</span><span class="na">toInstant</span><span class="o">().</span><span class="na">atZone</span><span class="o">(</span><span class="n">zoneId</span><span class="o">);</span>
<span class="o">}</span>
</code></pre></div></div>

<ul>
  <li>ZonedDateTime -&gt; Date</li>
</ul>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">public</span> <span class="kd">static</span> <span class="nc">Date</span> <span class="nf">convert</span><span class="o">(</span><span class="nc">ZonedDateTime</span> <span class="n">zonedDateTime</span><span class="o">)</span> <span class="o">{</span>
  <span class="k">return</span> <span class="nc">Date</span><span class="o">.</span><span class="na">from</span><span class="o">(</span><span class="n">zonedDateTime</span><span class="o">.</span><span class="na">toInstant</span><span class="o">());</span>
<span class="o">}</span>
</code></pre></div></div>

<h3 id="formatting">Formatting</h3>

<ul>
  <li>Date -&gt; String</li>
</ul>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">SimpleDateFormat</span> <span class="no">DATE_FORMAT</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">SimpleDateFormat</span><span class="o">(</span><span class="s">"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"</span><span class="o">,</span> <span class="nc">Locale</span><span class="o">.</span><span class="na">getDefault</span><span class="o">());</span>

<span class="kd">public</span> <span class="nc">String</span> <span class="nf">dateToISOString</span><span class="o">(</span><span class="nd">@Nonnull</span> <span class="nc">Date</span> <span class="n">date</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">return</span> <span class="no">DATE_FORMAT</span><span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="n">date</span><span class="o">);</span>
<span class="o">}</span>
</code></pre></div></div>

<ul>
  <li>String -&gt; Date</li>
</ul>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">SimpleDateFormat</span> <span class="no">DATE_FORMAT</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">SimpleDateFormat</span><span class="o">(</span><span class="s">"yyyy-MM-dd"</span><span class="o">,</span> <span class="nc">Locale</span><span class="o">.</span><span class="na">getDefault</span><span class="o">());</span>

<span class="kd">public</span> <span class="nc">Date</span> <span class="nf">dateFromString</span><span class="o">(</span><span class="nc">String</span> <span class="n">dateStr</span><span class="o">)</span> <span class="o">{</span> <span class="c1">// "2022-01-01"</span>
	<span class="k">return</span> <span class="no">DATE_FORMAT</span><span class="o">.</span><span class="na">parse</span><span class="o">(</span><span class="n">dateStr</span><span class="o">);</span> 
<span class="o">}</span>
</code></pre></div></div>

<h4 id="javatimedatetimeformatter">java.time.DateTimeFormatter</h4>

<p>Java 8의 DateTimeFormatter 클래스에서는 format과 관련된 풍부한 기능을 제공합니다. <code class="language-plaintext highlighter-rouge">DateTimeFormatter.ofPattern</code> 를 통해 직접 format을 설정할 수도 있고 이미 제공되는 여러 format을 활용할 수도 있습니다.</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">DateTimeFormatter</span><span class="o">.</span><span class="na">ofPattern</span><span class="o">(</span><span class="s">"yyyy-MM-dd HH:mm:ss"</span><span class="o">)</span>

<span class="nc">DateTimeFormatter</span><span class="o">.</span><span class="na">BASIC_ISO_DATE</span><span class="o">;</span> <span class="c1">//  '20111203'</span>
<span class="nc">DateTimeFormatter</span><span class="o">.</span><span class="na">ISO_DATE</span><span class="o">;</span> <span class="c1">// '2011-12-03' or '2011-12-03+01:00'</span>
<span class="nc">DateTimeFormatter</span><span class="o">.</span><span class="na">ISO_LOCAL_DATE</span><span class="o">;</span> <span class="c1">//  '2011-12-03'</span>
<span class="nc">DateTimeFormatter</span><span class="o">.</span><span class="na">ISO_LOCAL_TIME</span><span class="o">;</span> <span class="c1">//  '10:15' or '10:15:30'</span>
<span class="nc">DateTimeFormatter</span><span class="o">.</span><span class="na">ISO_LOCAL_DATE_TIME</span><span class="o">;</span> <span class="c1">//  '2011-12-03T10:15:30'</span>
<span class="nc">DateTimeFormatter</span><span class="o">.</span><span class="na">ISO_OFFSET_DATE_TIME</span><span class="o">;</span> <span class="c1">// '2011-12-03T10:15:30+01:00'</span>
<span class="nc">DateTimeFormatter</span><span class="o">.</span><span class="na">ISO_ZONED_DATE_TIME</span><span class="o">;</span> <span class="c1">// '2011-12-03T10:15:30+01:00[Europe/Paris]'</span>
</code></pre></div></div>

<ul>
  <li>LocalDate, LocalDateTime -&gt; String</li>
</ul>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&lt;</span><span class="nc">LocalDate</span><span class="o">&gt;.</span><span class="na">format</span><span class="o">(</span><span class="nc">DateTimeFormatter</span><span class="o">.</span><span class="na">BASIC_ISO_DATE</span><span class="o">);</span>
<span class="o">&lt;</span><span class="nc">LocalDateTime</span><span class="o">&gt;.</span><span class="na">format</span><span class="o">(</span><span class="nc">DateTimeFormatter</span><span class="o">.</span><span class="na">ofPattern</span><span class="o">(</span><span class="s">"yyyy-MM-dd HH:mm:ss"</span><span class="o">));</span>
</code></pre></div></div>

<ul>
  <li>String -&gt; LocalDate, LocalDateTime</li>
</ul>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">LocalDate</span><span class="o">.</span><span class="na">parse</span><span class="o">(</span><span class="s">"1995-05-09"</span><span class="o">);</span>
<span class="nc">LocalDate</span><span class="o">.</span><span class="na">parse</span><span class="o">(</span><span class="s">"20191224"</span><span class="o">,</span> <span class="nc">DateTimeFormatter</span><span class="o">.</span><span class="na">BASIC_ISO_DATE</span><span class="o">);</span> 

<span class="nc">LocalDateTime</span><span class="o">.</span><span class="na">parse</span><span class="o">(</span><span class="s">"2019-12-25T10:15:30"</span><span class="o">);</span>
<span class="nc">LocalDateTime</span><span class="o">.</span><span class="na">parse</span><span class="o">(</span><span class="s">"2019-12-25 12:30:00"</span><span class="o">,</span> <span class="nc">DateTimeFormatter</span><span class="o">.</span><span class="na">ofPattern</span><span class="o">(</span><span class="s">"yyyy-MM-dd HH:mm:ss"</span><span class="o">));</span>
</code></pre></div></div>

<ul>
  <li>ZonedDateTime -&gt; String</li>
</ul>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&lt;</span><span class="nc">ZonedDateTime</span><span class="o">&gt;.</span><span class="na">format</span><span class="o">(</span><span class="nc">DateTimeFormatter</span><span class="o">.</span><span class="na">ISO_ZONED_DATE_TIME</span><span class="o">);</span> <span class="c1">// ex. 2022-03-12T17:26:05.840250+09:00[Asia/Seoul]</span>
<span class="o">&lt;</span><span class="nc">ZonedDateTime</span><span class="o">&gt;.</span><span class="na">format</span><span class="o">(</span><span class="nc">DateTimeFormatter</span><span class="o">.</span><span class="na">ISO_OFFSET_DATE_TIME</span><span class="o">);</span> <span class="c1">// ex. 2022-03-12T17:26:05.840250+09:00</span>
</code></pre></div></div>

<ul>
  <li>String -&gt; ZonedDateTime</li>
</ul>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">String</span> <span class="n">input</span> <span class="o">=</span> <span class="s">"2022-03-12T17:26:05.840250+09:00[Asia/Seoul]"</span>
<span class="nc">ZonedDateTime</span><span class="o">.</span><span class="na">parse</span><span class="o">(</span><span class="n">input</span><span class="o">);</span>
</code></pre></div></div>

<h2 id="preserve-date-time-consistency-in-application">Preserve Date, Time Consistency in Application</h2>

<p>어플리케이션이 가지고 있는 대부분의 기능은 시간과 관련이 있습니다. 예를 들어 Uber의 배차 시스템에서 고객에게 드라이버를 매칭하기 위해서는 고객이 언제 배차 요청을 했는지, 드라이버들 중에서 후보 드라이버는 얼만큼 대기하고 있었는지, 기대 소요시간은 얼마인지 등 여러 시간 데이터들을 가지고 배차 시스템이 동작합니다.</p>

<p>하나의 어플리케이션은 다수의 웹 서버, 어플리케이션 서버, 데이터베이스 등 다양한 시스템이 긴밀하게 연결되어있는데요. 어플리케이션에 제대로 동작하려면 이들 시스템 간에 시간 데이터를 주고 받거나 영속적으로 저장할 때 시간 데이터의 일관성을 지키는 것이 매우 중요합니다. 그렇지 않으면 시간 역전 등의 문제로 인해 예측 및 분석하기가 어려운 장애가 발생할 수 있습니다. 일관성을 지키기 위해서는 다음의 두가지 규칙을 지켜야 합니다.</p>

<ul>
  <li>시간 데이터를 주고 받을 때 표준을 통해 주고 받습니다. (ISO-8601)</li>
  <li>시간 데이터를 저장할 때 요청 서버와 데이터베이스의 시간대 차이를 고려하여 시간을 변환하여 저장합니다.</li>
</ul>

<h3 id="iso-8601">ISO-8601</h3>

<p>ISO-8601은 date, time 관련 데이터들을 주고 받기 위한 국제 표준입니다. 잘 정의된 명확한 포맷을 통해서 날짜 및 시간 데이터를 주고 받을 때 잘못 해석되는 것을 방지하기 위한 표준이며 1988년에 처음 도입된 이후로 몇번의 개정이 있었습니다.</p>

<p>날짜, 주, 일, 등 시간을 나타내는 표준이 각각 존재하지만, 특정 지역에서의 시간대를 포함하여 시간을 표현하기 위해서 ISO-8601에서는 아래 포맷을 사용합니다. 시간에 관한 전체 정보를 모든 시스템이 동의하는 표준으로 주고 받으므로 직렬화, 역직렬화 중에도 데이터가 손실되거나 왜곡될 일 없이 일관성을 유지할 수 있습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>yyyy-MM-dd'T'HH:mm:ss'Z' (UTC)
yyyy-MM-dd'T'HH:mm:ss+HH:ss (offset)
</code></pre></div></div>

<h3 id="database">Database</h3>

<p>서버들 간에 시간 데이터의 일관성을 지키기 위해서 ISO-8601 표준을 지키는 것과 더불어 데이터베이스에서는 시간 데이터를 저장하고 이후에 서버의 요청으로부터 응답을 내려줄 때도 일관성을 지킬 책임이 있습니다.</p>

<p>MySQL에서는 요청 서버와 데이터베이스 서버의 시간대, 그리고 사용자 정의 시간대를 고려하여 시간을 변환하여 저장하기 위한 설정을 제공합니다. MySQL에서 시간 데이터를 어떻게 저장하는지에 대해서는 아래 글에서 추가적으로 알아보겠습니다.</p>

<p>(더보기)</p>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://medium.com/javarevisited/the-evolution-of-the-java-date-time-api-bfdc61375ddb">https://medium.com/javarevisited/the-evolution-of-the-java-date-time-api-bfdc61375ddb</a></li>
  <li><a href="https://docs.oracle.com/javase/8/docs/api/java/time/package-summary.html">https://docs.oracle.com/javase/8/docs/api/java/time/package-summary.html</a></li>
  <li><a href="https://d2.naver.com/helloworld/645609">Naver D2 - Java의 날짜와 시간 API</a></li>
  <li><a href="https://en.wikipedia.org/wiki/ISO_8601">https://en.wikipedia.org/wiki/ISO_8601</a></li>
  <li><a href="https://dev.mysql.com/doc/connector-j/8.0/en/connector-j-time-instants.html">https://dev.mysql.com/doc/connector-j/8.0/en/connector-j-time-instants.html</a></li>
</ul>]]></content><author><name>taehyeok-jang</name></author><category term="java" /><category term="java" /><category term="date" /><category term="time" /><category term="timezone" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">MySQL Preserve Date-Time Coherence</title><link href="/jekyll-theme-yat/database/2022/01/15/mysql-preserve-date-time-coherence.html" rel="alternate" type="text/html" title="MySQL Preserve Date-Time Coherence" /><published>2022-01-15T00:00:00+00:00</published><updated>2022-01-15T00:00:00+00:00</updated><id>/jekyll-theme-yat/database/2022/01/15/mysql-preserve-date-time-coherence</id><content type="html" xml:base="/jekyll-theme-yat/database/2022/01/15/mysql-preserve-date-time-coherence.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>어플리케이션이 가지고 있는 대부분의 기능은 시간과 관련이 있으므로 데이터 보관을 위한 데이터베이스에서는 시간 데이터의 일관성을 지킬 책임이 있습니다. 시간 데이터는 time instant라고도 표현하며 time zone에 상관 없이 타임라인 상의 동일한 시각을 가리킵니다. 따라서 시간 데이터의 일관성을 유지하는 것은 데이터를 저장하고 조회하는 과정에서 시간 데이터가 여전히 동일한 시각을 가리키도록 하는 것을 의미합니다. 이는 데이터베이스 서버와 클라이언트 간, 요청 클라이언트와 응답 클라이언트 간 time zone이 서로 다른 상황에서도 동일하게 요구됩니다.</p>

<p>MySQL에서는 시간 데이터의 일관성을 유지하기 위한 여러 설정들을 제공합니다. 사용자는 이 설정들을 통해 여러 시스템 간 시간대의 차이를 고려하여 주어진 시간 데이터를 적절하게 변환하여 저장할 수 있습니다. 이번 글에서는 MySQL에서 시간 데이터를 일관성있게 저장하는 방법에 대해서 알아보겠습니다.</p>

<h2 id="time-zones-went-through">Time Zones Went Through</h2>

<p>데이터베이스에 데이터를 저장하기까지 다양한 time zone이 관여합니다. Java 어플리케이션으로부터 MySQL 서버에 이르기까지 관여하는 time zone과 관련된 설정들은 아래와 같습니다.</p>

<h3 id="application-client-time-zone">Application, Client Time Zone</h3>

<ul>
  <li>Original time zone</li>
</ul>

<p>어플리케이션에서 생성되는 데이터의 time zone 입니다. java.util.Date의 경우 default로 JVM time zone이 적용되지만 java.util.Calendar, java.time.OffsetDateTime and java.time.ZonedDateTime 의 경우 명시적으로 time zone을 가지고 있습니다.</p>

<ul>
  <li>Client local time zone</li>
</ul>

<p>어플리케이션 서버의 JVM default time zone 입니다. 기본적으로 호스트의 system time zone과 동일하지만 어플리케이션에서 별도로 설정 가능합니다.</p>

<h3 id="mysql-time-zone">MySQL Time Zone</h3>

<ul>
  <li>MySQL TIMESTAMP internal time zone</li>
</ul>

<p>MySQL에서 시간 데이터를 저장하기 위한 기준 time zone입니다. 특정 time zone의 구분 없이 항상 UTC로 변환하여 저장합니다.</p>

<ul>
  <li>MySQL system time zone</li>
</ul>

<p>MySQL 서버의 system time zone입니다. 서버를 기동 시 결정하여 <code class="language-plaintext highlighter-rouge">system_time_zone</code> 이라는 시스템 변수로 사용합니다. 서버를 시작할 때 호스트 머신의 기본 설정을 그대로 상속하며 특정 사용자의 설정이나 시작 스크립트를 통해 수정할 수 있습니다.</p>

<ul>
  <li>MySQL current (GLOBAL) time zone</li>
</ul>

<p>현재 MySQL 서버가 동작하고 있는 전역 time zone입니다. 초기값은 <code class="language-plaintext highlighter-rouge">SYSTEM</code>이며 이는 system time zone과 동일한 값을 사용하고 있다는 의미입니다. 전역 time zone은 MySQL 기동 시 <code class="language-plaintext highlighter-rouge">default-time-zone</code> option이나 my.cnf 파일을 통해 설정할 수 있으며 시스템 권한이 있으면 아래와 같이 쿼리문을 통해 변경할 수 도 있습니다.</p>

<div class="language-ini highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="py">default-time-zone</span><span class="p">=</span><span class="s">'timezone'</span> <span class="s">// ex. '+09:00'</span>
</code></pre></div></div>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SET</span> <span class="k">GLOBAL</span> <span class="n">time_zone</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">timezone</span><span class="o">&gt;</span><span class="p">;</span>
<span class="n">Ex</span><span class="p">.</span> 
<span class="k">SET</span> <span class="o">@@</span><span class="k">global</span><span class="p">.</span><span class="n">time_zone</span> <span class="o">=</span> <span class="s1">'+00:00'</span><span class="p">;</span>
<span class="k">SET</span> <span class="k">GLOBAL</span> <span class="n">time_zone</span> <span class="o">=</span> <span class="s1">'+09:00'</span><span class="p">;</span>
<span class="k">SET</span> <span class="k">GLOBAL</span> <span class="n">time_zone</span> <span class="o">=</span> <span class="s1">'Asia/Seoul'</span><span class="p">;</span>


<span class="k">SELECT</span> <span class="o">@@</span><span class="k">global</span><span class="p">.</span><span class="n">time_zone</span><span class="p">;</span>
</code></pre></div></div>

<ul>
  <li>MySQL per-session time zones</li>
</ul>

<p>MySQL 서버에 연결된 클라이언트 별 session time zone이다. 초기값은 <code class="language-plaintext highlighter-rouge">SYSTEM</code> 이며 서버 기동 시  <code class="language-plaintext highlighter-rouge">default-time-zone</code> option을 통해 설정할 수 있습니다. 아래에서 자세히 다루겠지만 클라이언트가 MySQL 서버에 connection을 생성할 때 connection properties을 통해서도 설정 가능합니다.</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SET</span> <span class="n">time_zone</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">timezone</span><span class="o">&gt;</span><span class="p">;</span>
<span class="n">Ex</span><span class="p">.</span> 
<span class="k">SET</span> <span class="n">time_zone</span> <span class="o">=</span> <span class="s1">'Europe/Helsinki'</span><span class="p">;</span>
<span class="k">SET</span> <span class="n">time_zone</span> <span class="o">=</span> <span class="nv">"+00:00"</span><span class="p">;</span>
<span class="k">SET</span> <span class="o">@@</span><span class="k">session</span><span class="p">.</span><span class="n">time_zone</span> <span class="o">=</span> <span class="nv">"+00:00"</span><span class="p">;</span>

<span class="k">SELECT</span> <span class="o">@@</span><span class="k">session</span><span class="p">.</span><span class="n">time_zone</span><span class="p">;</span>
</code></pre></div></div>

<h2 id="mysql-connectorj-connnection-properties-version-80">MySQL Connector/J Connnection Properties (version 8.0~)</h2>

<p>MySQL의 공식 JDBC driver인 MySQL Connector/J에서는 클라이언트와 MySQL 서버 간 시간 데이터를 주고 받는 것과 관련된 설정을 제공합니다. 클라이언트에서 connection을 생성할 때 설정이 적용되며, 각 설정들을 잘 조합하여 일관성을 유지하기 위한 적절한 방법을 만들어 낼 수 있습니다.</p>

<h3 id="connectiontimezone">connectionTimeZone</h3>

<p>connectionTimeZone은 MySQL Connector/J가 어떻게 session time zone을 결정할지를 정하는 변수이며 클라이언트와 MySQL 서버 간 time zone에 따른 시간 변환을 어떻게 할지를 Connector/J에게 알려줍니다. 이전 설정인 serverTimeZone에서 명칭이 바뀐 것이며 MySQL global 혹은 session time zone과 꼭 같지 않아도 된다는 것을 강조하였습니다.</p>

<ul>
  <li>LOCAL (default)</li>
</ul>

<p>connectionTimeZone을 JVM default time zone과 같게 하겠다는 것을 의미합니다.</p>

<ul>
  <li>SERVER</li>
</ul>

<p>Connector/J가 session time zone을 MySQL 서버 설정인 time_zone으로 하겠다는 의미입니다.</p>

<ul>
  <li>user-defined time zone</li>
</ul>

<p>사용자 정의 time zone 설정이 가능하며 time zone 입력은 ZoneId에서 사용되는 syntax 형태를 따라야합니다.</p>

<p>유의해야 할 점은 이 설정만으로 MySQL 서버의 session time zone 설정이 바뀌지는 않으며 변경하려면 forceConnectionTimeZoneToSession 값을 true로 설정해야한다. 마찬가지로 preserveInstants 값이 false이면 시간 데이터의 time zone에 따른 변환이 전혀 이루어지지 않습니다.</p>

<h3 id="forceconnectiontimezonetosession">forceConnectionTimeZoneToSession</h3>

<p>forceConnectionTimeZoneToSession은 session time zone을 connectionTimeZone에서 설정한 값으로 설정할지 말지를 결정하는 변수입니다.</p>

<ul>
  <li>false (default)</li>
</ul>

<p>session time zone이 MySQL 서버 상에서 바뀌지 않습니다.</p>

<ul>
  <li>true</li>
</ul>

<p>driver는 session time zone 값이 connectionTimeZone으로 설정한 값으로 변경됩니다.</p>

<p>참고로 이 설정을 connectionTimeZone=SERVER와 함께 사용하면 아무런 효과가 없습니다. MySQL의 time zone은 이미 MySQL 서버 설정으로 되어있기 때문입니다.</p>

<h3 id="preserveinstants">preserveInstants</h3>

<p>preserveInstants은 클라이언트와 MySQL 서버 간 시간 데이터를 주고 받을 때 타임라인 상에서 동일한 시점을 영속적으로 가리키기 위해 시간 변환을 하도록 지시하는 변수입니다. Java의 time instant 기반 변수인 java.sql.Timestamp, java.time.OffsetDateTime 두 클래스에 대해서 1) 저장할 때는 MySQL의 대상 칼럼의 타입이 TIMESTAMP일 때, 2) 조회할 때는 MySQL의 대상 칼럼의 타입이 TIMESTAMP, DATETIME일 때, 클라이언트와 MySQL 서버 간 time zone 차이만큼 시간 변환을 합니다.</p>

<ul>
  <li>true (default)</li>
</ul>

<p>Connector/J는 위 connectionTimeZone, forceConnectionTimeZoneToSession 변수를 고려하여 시간 변환을 합니다.</p>

<ul>
  <li>false</li>
</ul>

<p>시간 변환이 이루어지지 않으며 서버로부터 timestamp는 데이터베이스에 그대로 저장됩니다. 결과적으로 time instant는 실제 값을 유지하지 못하게 됩니다.</p>

<p>시간 변환을 하지 않을 때 time instant가 어떻게 왜곡되는지 예시를 들어보겠습니다.</p>

<ul>
  <li>Time zones: 클라이언트(JVM)는 UTC , server session은 UTC+1.</li>
  <li>클라이언트로부터 original timestamp (UTC): <code class="language-plaintext highlighter-rouge">2020-01-01 01:00:00</code></li>
  <li>Connector/J에 의해 MySQL 서버로 전송된 timestamp: <code class="language-plaintext highlighter-rouge">2020-01-01 01:00:00</code> (시간 변환 없음)</li>
  <li>MySQL 서버에 저장된 timestamp: <code class="language-plaintext highlighter-rouge">2020-01-01 00:00:00 UTC</code> (<code class="language-plaintext highlighter-rouge">2020-01-01 00:00:00 UTC+1</code> 에서 UTC로 내부 시스템에서 변환)</li>
  <li>MySQL 서버에서 조회할 때 server session (UTC+1)에서의 timestamp: : <code class="language-plaintext highlighter-rouge">2020-01-01 01:00:00</code>( <code class="language-plaintext highlighter-rouge">2020-01-01 00:00:00</code> UTC에서 UTC+1로 내부 시스템에서 변환)</li>
  <li>
    <p>이전 클라이언트와는 다른 시간대의 (UTC+3) Connector/J로 연결된 클라이언트(JVM)에서의 timestamp: <code class="language-plaintext highlighter-rouge">2020-01-01 01:00:00</code> (새로운 클라이언트의 time zone과 상관 없이 그대로 반환)</p>
  </li>
  <li>=&gt; time instant의 일관성이 지켜지지 않음.</li>
</ul>

<h2 id="preserving-time-instants">Preserving Time Instants</h2>

<p>앞서 언급한대로 MySQL Connector/J의 여러 설정들을 통해 시간 데이터의 일관성을 유지할 수 있습니다. MySQL에서는 시간 데이터를 저장할 때 original time zone을 포함하지 않으며, 그 대신에 session time zone 상에서 적절하게 표시된다고 가정합니다. 이는 시간 데이터를 저장하기 이전에 클라이언트와 MySQL 서버 간 시간 변환이 필요하다는 것을 의미합니다. 어플리케이션이 처한 상황에 맞추어 Connector/J의 설정들을 정하는 몇가지 방법을 알아보겠습니다. 시간 변환이 언제 발생하는지에 초점을 두면 이해하기가 한결 수월합니다.</p>

<h3 id="soution-1">Soution 1.</h3>

<p><img src="https://user-images.githubusercontent.com/31732943/162183693-cb35327e-cfbd-4f08-9191-d3aa8172aedc.png" alt="mysql_datetime_solution_1" style="zoom:67%;" /></p>

<p>connectionTimeZone=LOCAL&amp;forceConnectionTimeZoneToSession=false</p>

<ul>
  <li>Time zones: 클라이언트(JVM)와 server session 모두 UTC+1.</li>
  <li>클라이언트로부터 original timestamp (UTC): <code class="language-plaintext highlighter-rouge">2020-01-01 01:00:00</code></li>
  <li>Connector/J에 의해 MySQL 서버로 전송된 timestamp: <code class="language-plaintext highlighter-rouge">2020-01-01 01:00:00</code> (시간 변환 없음)</li>
  <li>MySQL 서버에 저장된 timestamp: <code class="language-plaintext highlighter-rouge">2020-01-01 00:00:00 UTC</code> (<code class="language-plaintext highlighter-rouge">2020-01-01 01:00:00 UTC+1</code> 에서 UTC로 내부 시스템에서 변환)</li>
  <li>MySQL 서버에서 조회할 때 server session (UTC+1)에서의 timestamp: : <code class="language-plaintext highlighter-rouge">2020-01-01 01:00:00</code>( <code class="language-plaintext highlighter-rouge">2020-01-01 00:00:00</code> UTC에서 UTC+1로 내부 시스템에서 변환)</li>
  <li>동일 클라이언트(JVM) (UTC+1)에서의 timestamp: <code class="language-plaintext highlighter-rouge">2020-01-01 01:00:00</code></li>
</ul>

<p>=&gt; 변환 없이 time instant의 일관성이 유지됨.</p>

<h3 id="solution-2">Solution 2.</h3>

<p><img src="https://user-images.githubusercontent.com/31732943/162183738-4dee6ff0-e82b-4051-a4d0-5c0927bdba2c.png" alt="mysql_datetime_solution_2" style="zoom:67%;" /></p>

<p>connectionTimeZone=LOCAL&amp; forceConnectionTimeZoneToSession=true</p>

<ul>
  <li>Time zones: 클라이언트(JVM)는 UTC+1, server session은 UTC+2 이었으나 Connector/J에 의해 UTC+1로 변경.</li>
  <li>클라이언트로부터 original timestamp (UTC+1): <code class="language-plaintext highlighter-rouge">2020-01-01 01:00:00</code></li>
  <li>Connector/J에 의해 MySQL 서버로 전송된 timestamp: <code class="language-plaintext highlighter-rouge">2020-01-01 01:00:00</code> (시간 변환 없음)</li>
  <li>MySQL 서버에 저장된 timestamp: <code class="language-plaintext highlighter-rouge">2020-01-01 00:00:00 UTC</code> (<code class="language-plaintext highlighter-rouge">2020-01-01 01:00:00 UTC+1</code> 에서 UTC로 내부 시스템에서 변환)</li>
  <li>MySQL 서버에서 조회할 때 server session (UTC+1)에서의 timestamp: : <code class="language-plaintext highlighter-rouge">2020-01-01 01:00:00</code>( <code class="language-plaintext highlighter-rouge">2020-01-01 00:00:00</code> UTC에서 UTC+1로 내부 시스템에서 변환)</li>
  <li>동일 클라이언트(JVM) (UTC+1)에서의 timestamp: <code class="language-plaintext highlighter-rouge">2020-01-01 01:00:00</code> (변환 없음)</li>
  <li>Connector/J에 의해 변경된 time zone (UTC+3)의 server session에서 timestamp: <code class="language-plaintext highlighter-rouge">2020-01-01 03:00:00</code></li>
  <li>변경된 클라이언트 (UTC+3) (JVM)에서의 timestamp: <code class="language-plaintext highlighter-rouge">2020-01-01 03:00:00</code> (변환 없음)</li>
</ul>

<p>=&gt; Time instant is preserved without conversion by Connector/J, because the session time zone is changed by Connector/J to its JVM’s value.</p>

<h3 id="solution-3">Solution 3.</h3>

<p><img src="https://user-images.githubusercontent.com/31732943/162183757-8804eac3-49fa-46cf-8b9e-88284980b937.png" alt="mysql_datetime_solution_3" style="zoom: 67%;" /></p>

<p>preserveInstants=true&amp;connectionTimeZone=<user-defined-time-zone>&amp; forceConnectionTimeZoneToSession=true</user-defined-time-zone></p>

<p>이 방법은 Connector/J에 의해 서버의 session time zone 설정을 인식하는 것이 불가능할 때 (CST, CEST 등) 주로 사용됩니다.</p>

<ul>
  <li>Time zones: 클라이언트(JVM)는 UTC+2, server session은 CET이었으나 Connector/J에 의해 user-specified <code class="language-plaintext highlighter-rouge">Europe/Berlin</code> 으로 변경.</li>
  <li>클라이언트로부터 original timestamp (UTC+2): <code class="language-plaintext highlighter-rouge">2020-01-01 02:00:00</code></li>
  <li>Connector/J에 의해 MySQL 서버로 전송된 timestamp: <code class="language-plaintext highlighter-rouge">2020-01-01 01:00:00</code> (JVM time zone (UTC+2)과 user-defined time zone (<code class="language-plaintext highlighter-rouge">Europe/Berlin</code>=UTC+1) 간 변경)</li>
  <li>MySQL 서버에 저장된 timestamp: <code class="language-plaintext highlighter-rouge">2020-01-01 00:00:00 UTC</code> (<code class="language-plaintext highlighter-rouge">2020-01-01 01:00:00 UTC+1</code> 에서 UTC로 내부 시스템에서 변환)</li>
  <li>MySQL 서버에서 조회할 때 server session (UTC+1)에서의 timestamp: : <code class="language-plaintext highlighter-rouge">2020-01-01 01:00:00</code>(UTC에서 <code class="language-plaintext highlighter-rouge">Europe/Berlin</code> (UTC+1)로 내부 시스템에서 변환)</li>
  <li>동일 클라이언트(JVM) (UTC+2) 어플리케이션에서의 timestamp: <code class="language-plaintext highlighter-rouge">2020-01-01 02:00:00</code> (user-defined time zone (UTC+1) and JVM time zone (UTC+2) 간 변환)</li>
</ul>

<p>=&gt; Time instant is preserved with conversion and with the session time zone being changed by Connector/J according to a user-defined value.</p>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://dev.mysql.com/doc/refman/8.0/en/time-zone-support.html">https://dev.mysql.com/doc/refman/8.0/en/time-zone-support.html</a></li>
  <li><a href="https://dev.mysql.com/doc/connector-j/8.0/en/connector-j-time-instants.html">https://dev.mysql.com/doc/connector-j/8.0/en/connector-j-time-instants.html</a></li>
  <li><a href="https://dev.mysql.com/blog-archive/support-for-date-time-types-in-connector-j-8-0/">https://dev.mysql.com/blog-archive/support-for-date-time-types-in-connector-j-8-0/</a></li>
  <li><a href="https://phoenixnap.com/kb/change-mysql-time-zone">https://phoenixnap.com/kb/change-mysql-time-zone</a></li>
  <li><a href="https://dev.mysql.com/doc/connector-j/8.0/en/connector-j-connp-props-datetime-types-processing.html">https://dev.mysql.com/doc/connector-j/8.0/en/connector-j-connp-props-datetime-types-processing.html</a></li>
</ul>]]></content><author><name>taehyeok-jang</name></author><category term="database" /><category term="mysql" /><category term="date" /><category term="time" /><category term="timezone" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">API Design in Distributed Systems</title><link href="/jekyll-theme-yat/distributed-systems/2022/01/03/api-design-in-distributed-systems.html" rel="alternate" type="text/html" title="API Design in Distributed Systems" /><published>2022-01-03T00:00:00+00:00</published><updated>2022-01-03T00:00:00+00:00</updated><id>/jekyll-theme-yat/distributed-systems/2022/01/03/api-design-in-distributed-systems</id><content type="html" xml:base="/jekyll-theme-yat/distributed-systems/2022/01/03/api-design-in-distributed-systems.html"><![CDATA[<p>이 글은 Stripe Engineering Blog의 <a href="https://stripe.com/blog/idempotency">Designing robust and predictable APIs with idempotency</a> article과 관련 자료를 읽고 정리한 글입니다.</p>

<h2 id="introduction">Introduction</h2>

<blockquote>
  <p>Networks are unreliable. We’ve all experienced trouble connecting to Wi-Fi, or had a phone call drop on us abruptly.</p>

  <p>The networks connecting our servers are, on average, more reliable than consumer-level last miles like cellular or home ISPs, but given enough information moving across the wire, they’re still going to fail in exotic ways. Outages, routing problems, and other intermittent failures may be statistically unusual on the whole, but still bound to be happening all the time at some ambient background rate.</p>

  <p>To overcome this sort of inherently unreliable environment, it’s important to design APIs and clients that will be robust in the event of failure, and will predictably bring a complex integration to a consistent state despite them. Let’s take a look at a few ways to do that.</p>
</blockquote>

<p>network는 unreliable 합니다. celluar network나 home ISP보다는 신뢰할 수 있지만, network 문제는 전체적으로 보면 통계적으로 unusual 하다고 하더라도 여전히 유효한 비율로 발생할 것입니다.</p>

<p>inherently unreliable environment, 즉 본질적으로 unreliable한 환경에서 API와 client들을 설계하는 것은 매우 중요합니다. 이를 위해 특정한 사건의 실패에 robust하고, 실패에도 불구하고 일관성 있는 데이터를 위한 통합 작업을 예측적으로 수행해야 합니다.</p>

<h2 id="planning-for-failure">Planning for failure</h2>

<blockquote>
  <p>Consider a call between any two nodes. There are a variety of failures that can occur:</p>

  <p>- The initial connection could fail as the client tries to connect to a server.</p>

  <p>- The call could fail midway while the server is fulfilling the operation, leaving the work in limbo.</p>

  <p>- The call could succeed, but the connection break before the server can tell its client about it.</p>
</blockquote>

<p>network application, 즉 네트워크를 기반으로 하는 인터넷 상의 모든 서비스들은 네트워크 실패에 대비해야 합니다. 기본적으로 network 상의 두 node가 있다고 가정했을 때 위와 같은 실패 가능성이 있습니다. 이와 같이 네트워크는 본질적으로 실패하며 어플리케이션은 이 실패에 대응할 방법을 계획해야 합니다.</p>

<ul>
  <li>서버에 도착하기 전 연결 실패</li>
  <li>서버 처리 도중 실패 (client abort, timeout 등)</li>
  <li>서버에서 응답을 내려주는 도중 연결 중단 (infrastructure aborted)</li>
</ul>

<h2 id="guaranteeing-exactly-once-semantics">Guaranteeing “exactly once” semantics</h2>

<blockquote>
  <p>While the inherently idempotent HTTP semantics around PUT and DELETE are a good fit for many API calls, what if we have an operation that needs to be invoked exactly once and no more? An example might be if we were designing an API endpoint to charge a customer money; accidentally calling it twice would lead to the customer being double-charged, which is very bad.</p>

  <p>This is where <em>idempotency keys</em> come into play. When performing a request, a client generates a unique ID to identify just that operation and sends it up to the server along with the normal payload. The server receives the ID and correlates it with the state of the request on its end. If the client notices a failure, it retries the request with the same ID, and from there it’s up to the server to figure out what to do with it.</p>
</blockquote>

<p>네트워크 실패에 대처하기 위해서 idempotent semantic은 유용합니다. 왜냐하면 클라이언트가 실패를 인지한 상황에서 재시도 요청을 했을 때 여러번 호출하더라도 동일한 결과를 보장하기 때문입니다. 반대로 ‘increment by 1’과 같은 요청을 보낸다면 네트워크 실패 시에 +1이 될지 혹은 +2, +3가 될지 예측하기 어렵습니다.</p>

<p>HTTP의 PUT, DELETE method는 이미 idempotent semantic을 내포하고 있지만, POST와 같은 method에서 특정 operation이 정확하게 한번만 일어나야 한다는 것이 필요하다면 어떻게 해야할까요?</p>

<p>여기서 idempontency key를 도입합니다. client에서 unique 한 indempotent key를 생성하여 요청 시 함께 전송하고 서버에서는 이를 고려하여 요청을 처리합니다. 그러면 위 네트워크 실패 시나리오에서 다음과 같이 대처할 수 있습니다.</p>

<ul>
  <li>서버에 도착하기 전 연결 실패</li>
</ul>

<p>클라이언트 재시도 시에 서버는 해당 ID가 포함된 요청이 처음이므로 정상적으로 처리합니다.</p>

<ul>
  <li>서버 처리 도중 실패</li>
</ul>

<p>정확한 구현은 어플리케이션에 따라 달라질 수 있지만 ACID 데이터베이스로 처리 도중 실패했다면 성공적으로 rollback이 되었을 것입니다. 따라서 동일한 ID를 가진 요청은 성공적으로 처리 될 것입니다.</p>

<ul>
  <li>서버에서 응답을 내려주는 도중 연결 중단</li>
</ul>

<p>서버는 해당 요청을 중복 요청으로 인식하고 캐시된 응답을 내려줍니다.</p>

<h2 id="being-a-good-distributed-citizen">Being a good distributed citizen</h2>

<blockquote>
  <p>Safely handling failure is hugely important, but beyond that, it’s also recommended that it be handled in a considerate way. When a client sees that a network operation has failed, there’s a good chance that it’s due to an intermittent failure that will be gone by the next retry. However, there’s also a chance that it’s a more serious problem that’s going to be more tenacious; for example, if the server is in the middle of an incident that’s causing hard downtime. Not only will retries of the operation not go through, but they may contribute to further degradation.</p>

</blockquote>

<p>네트워크 실패는 잠깐의 문제일 수도 있지만 지속적인 모니터링이 필요한 심각한 장애일 수도 있습니다. 따라서 무작정 재시도를 했다가는 분산 시스템의 네트워크 부하에 영향을 미칠 수도 있습니다.</p>

<p>따라서 재시도를 효율적으로 하기 위해서, TCP/IP congestion control에서의 방법과 마찬가지로 재시도의 주기를 늘려나가는 exponential backoff와 같은 전략을 사용할 수 있습니다.</p>

<p>또한 서버의 장애가 많은 수의 클라이언트에게 장애를 유발했을 때 thundering herd problem을 유발할 수 있다. 이 문제는 event 발생을 기다리고 있는 process or thread가 있으나 하나의 서버에서만 event handle이 가능할 때를 의미합니다. 리소스에 대해 경합할 것이고 순간적으로 해당 서버에 과부하가 걸리게 합니다.</p>

<p>이 문제를 해결하기 위해서 jitter 라는 randomness 개념을 도입할 수 있습니다. 재시도를 하려는 클라이언트들이 timeframe 내에서 임의의 시간에 호출하도록 하여 부하를 분산시키는 방법 있습니다.</p>

<h2 id="summary">Summary</h2>

<p>이 글의 요약입니다.</p>

<ol>
  <li>실패가 consistent하게 처리될 수 있도록 하기. 클라이언트에서 재시도를 하여 데이터를 inconsistent state에서 벗어나게 하기.</li>
  <li>idempontent한 서버의 설계를 통해 실패 처리가 안전하게 처리될 수 있도록.</li>
  <li>실패 처리가 전체 시스템에 responsible한 방법으로 처리될 수 있도록 하기. (exponential backoff and random jitter)</li>
</ol>

<h2 id="references">References</h2>

<ul>
  <li>Stripe Engineering Blog
    <ul>
      <li><a href="https://stripe.com/blog/idempotency">https://stripe.com/blog/idempotency</a></li>
      <li><a href="http://www.cs.utexas.edu/users/lam/NRL/backoff.html">http://www.cs.utexas.edu/users/lam/NRL/backoff.html</a></li>
    </ul>
  </li>
  <li>Designing Data-Intensive Applications
    <ul>
      <li><a href="https://dataintensive.net/">https://dataintensive.net/</a></li>
    </ul>
  </li>
  <li>AWS
    <ul>
      <li><a href="https://aws.amazon.com/builders-library/making-retries-safe-with-idempotent-APIs/">https://aws.amazon.com/builders-library/making-retries-safe-with-idempotent-APIs/</a></li>
      <li><a href="https://aws.amazon.com/builders-library/timeouts-retries-and-backoff-with-jitter/">https://aws.amazon.com/builders-library/timeouts-retries-and-backoff-with-jitter/</a></li>
    </ul>
  </li>
</ul>]]></content><author><name>taehyeok-jang</name></author><category term="distributed-systems" /><category term="api" /><category term="distributed-systems" /><summary type="html"><![CDATA[이 글은 Stripe Engineering Blog의 Designing robust and predictable APIs with idempotency article과 관련 자료를 읽고 정리한 글입니다.]]></summary></entry></feed>